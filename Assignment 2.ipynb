{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLaf6lUUQo4b",
        "outputId": "d357a39d-96ff-4538-9fbe-e336bcad7352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10  train_loss=1.7088  train_acc=40.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 40.37%\n",
            "  --> New best val acc: 40.37%  (checkpoint saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10  train_loss=1.5966  train_acc=43.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 46.03%\n",
            "  --> New best val acc: 46.03%  (checkpoint saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10  train_loss=1.5646  train_acc=44.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 45.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10  train_loss=1.5094  train_acc=46.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 45.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10  train_loss=1.4429  train_acc=48.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 46.81%\n",
            "  --> New best val acc: 46.81%  (checkpoint saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10  train_loss=1.3974  train_acc=50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 49.47%\n",
            "  --> New best val acc: 49.47%  (checkpoint saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10  train_loss=1.4049  train_acc=48.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 48.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10  train_loss=1.3611  train_acc=50.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 46.93%\n",
            "Switching to Stage 2: unfreezing backbone and creating new optimizer with param groups.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10  train_loss=1.2235  train_acc=55.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 54.17%\n",
            "  --> New best val acc: 54.17%  (checkpoint saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10  train_loss=1.0605  train_acc=61.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  val_acc (mean over 100 episodes) = 58.69%\n",
            "  --> New best val acc: 58.69%  (checkpoint saved)\n",
            "Training complete. Best val acc: 58.69%\n"
          ]
        }
      ],
      "source": [
        "# protonet_mobilenet_final_tqdm.py\n",
        "\"\"\"\n",
        "Prototypical Network with MobileNetV2 backbone (ImageNet pretrained),\n",
        "two-stage fine-tuning, cosine-prototype similarity with learnable temperature,\n",
        "and episodic training on CIFAR-100.\n",
        "\n",
        "This variant adds tqdm progress bars for training episodes, validation episodes,\n",
        "and meta-test episodes so you can observe progress in real time.\n",
        "\n",
        "Usage:\n",
        "    python protonet_mobilenet_final_tqdm.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Episode params\n",
        "TRAIN_N_WAY_MIN = 5\n",
        "TRAIN_N_WAY_MAX = 10\n",
        "K_SHOT = 1\n",
        "Q_QUERY = 15\n",
        "EPISODES_PER_EPOCH = 200\n",
        "EPOCHS = 10\n",
        "STAGE1_EPOCHS = 8\n",
        "\n",
        "EMBED_DIM = 256\n",
        "LR_HEAD = 1e-3\n",
        "LR_BACKBONE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Meta-test settings\n",
        "META_TEST_N = 10\n",
        "META_TEST_K = 1\n",
        "META_TEST_Q = 10\n",
        "META_TEST_EPISODES = 600\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset: episodic sampler for CIFAR-100\n",
        "# ----------------------------\n",
        "class EpisodicCIFAR100:\n",
        "    def __init__(self, root='./data', train=True, transform=None, train_class_split=0.8):\n",
        "        self.cifar = torchvision.datasets.CIFAR100(root=root, train=True, download=True)\n",
        "        self.transform = transform\n",
        "        by_class = defaultdict(list)\n",
        "        for idx, (_, label) in enumerate(self.cifar):\n",
        "            by_class[label].append(idx)\n",
        "        all_classes = sorted(list(by_class.keys()))\n",
        "        n_train_classes = int(len(all_classes) * train_class_split)\n",
        "        train_classes = all_classes[:n_train_classes]\n",
        "        val_classes = all_classes[n_train_classes:]\n",
        "        classes_use = train_classes if train else val_classes\n",
        "        self.by_class = {}\n",
        "        for new_label, cls in enumerate(classes_use):\n",
        "            self.by_class[new_label] = by_class[cls]\n",
        "        self.classes = list(self.by_class.keys())\n",
        "\n",
        "    def sample_episode(self, n_way, k_shot, q_query):\n",
        "        chosen_classes = random.sample(self.classes, n_way)\n",
        "        support_x, support_y, query_x, query_y = [], [], [], []\n",
        "        for i, cls in enumerate(chosen_classes):\n",
        "            indices = random.sample(self.by_class[cls], k_shot + q_query)\n",
        "            for si in indices[:k_shot]:\n",
        "                img, _ = self.cifar[si]\n",
        "                support_x.append(self.transform(img) if self.transform else img)\n",
        "                support_y.append(i)\n",
        "            for qi in indices[k_shot:]:\n",
        "                img, _ = self.cifar[qi]\n",
        "                query_x.append(self.transform(img) if self.transform else img)\n",
        "                query_y.append(i)\n",
        "        support_x = torch.stack(support_x)\n",
        "        query_x = torch.stack(query_x)\n",
        "        support_y = torch.tensor(support_y, dtype=torch.long)\n",
        "        query_y = torch.tensor(query_y, dtype=torch.long)\n",
        "        return support_x, support_y, query_x, query_y\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "class MobileNetProto(nn.Module):\n",
        "    def __init__(self, embed_dim=EMBED_DIM, pretrained=True):\n",
        "        super().__init__()\n",
        "        m = models.mobilenet_v2(pretrained=pretrained)\n",
        "        self.features = m.features\n",
        "        last_channels = m.last_channel\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(last_channels, last_channels // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(last_channels // 2, embed_dim)\n",
        "        )\n",
        "        self.log_scale = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        pooled = self.pool(f).view(f.size(0), -1)\n",
        "        emb = self.fc(pooled)\n",
        "        emb = F.normalize(emb, p=2, dim=1)\n",
        "        scale = torch.exp(self.log_scale)\n",
        "        return emb, scale\n",
        "\n",
        "# ----------------------------\n",
        "# Loss (cosine prototypical)\n",
        "# ----------------------------\n",
        "def prototypical_loss_cosine(support_emb, support_y, query_emb, query_y, n_way, k_shot, scale):\n",
        "    prototypes = []\n",
        "    for c in range(n_way):\n",
        "        idxs = (support_y == c).nonzero(as_tuple=True)[0]\n",
        "        proto = support_emb[idxs].mean(dim=0)\n",
        "        proto = F.normalize(proto, p=2, dim=0)\n",
        "        prototypes.append(proto)\n",
        "    prototypes = torch.stack(prototypes)\n",
        "    logits = scale * (query_emb @ prototypes.t())\n",
        "    log_p = F.log_softmax(logits, dim=1)\n",
        "    loss = F.nll_loss(log_p, query_y.to(log_p.device))\n",
        "    y_hat = log_p.argmax(dim=1)\n",
        "    acc = (y_hat == query_y.to(y_hat.device)).float().mean().item()\n",
        "    return loss, acc\n",
        "\n",
        "# ----------------------------\n",
        "# Training with progress bars\n",
        "# ----------------------------\n",
        "def train_protonet():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    episodic_train = EpisodicCIFAR100(train=True, transform=train_transform, train_class_split=0.8)\n",
        "    episodic_val = EpisodicCIFAR100(train=False, transform=val_transform, train_class_split=0.8)\n",
        "\n",
        "    model = MobileNetProto(embed_dim=EMBED_DIM, pretrained=True).to(device)\n",
        "\n",
        "    # Stage 1: freeze backbone\n",
        "    for p in model.features.parameters():\n",
        "        p.requires_grad = False\n",
        "    model.features.eval()\n",
        "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=12, gamma=0.5)\n",
        "\n",
        "    best_val = 0.0\n",
        "\n",
        "    # outer epoch loop with tqdm\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        accs = []\n",
        "\n",
        "        # inner episode loop with tqdm progress bar\n",
        "        ep_bar = tqdm(range(EPISODES_PER_EPOCH), desc=f\"Epoch {epoch}/{EPOCHS} episodes\", leave=False)\n",
        "        for _ in ep_bar:\n",
        "            n_way = random.randint(TRAIN_N_WAY_MIN, TRAIN_N_WAY_MAX)\n",
        "            support_x, support_y, query_x, query_y = episodic_train.sample_episode(n_way, K_SHOT, Q_QUERY)\n",
        "            support_x = support_x.to(device); support_y = support_y.to(device)\n",
        "            query_x = query_x.to(device); query_y = query_y.to(device)\n",
        "\n",
        "            support_emb, scale = model(support_x)\n",
        "            query_emb, _ = model(query_x)\n",
        "\n",
        "            loss, acc = prototypical_loss_cosine(support_emb, support_y, query_emb, query_y, n_way, K_SHOT, scale)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_norm=5.0)\n",
        "            opt.step()\n",
        "\n",
        "            losses.append(loss.item()); accs.append(acc)\n",
        "            # update tqdm postfix\n",
        "            ep_bar.set_postfix({\"loss\": f\"{loss.item():.3f}\", \"acc\": f\"{acc*100:.2f}%\"})\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_loss = sum(losses) / len(losses)\n",
        "        avg_acc = sum(accs) / len(accs)\n",
        "        tqdm.write(f\"Epoch {epoch}/{EPOCHS}  train_loss={avg_loss:.4f}  train_acc={avg_acc*100:.2f}%\")\n",
        "\n",
        "        # Validation with a progress bar\n",
        "        model.eval()\n",
        "        val_accs = []\n",
        "        val_episodes = 100\n",
        "        val_bar = tqdm(range(val_episodes), desc=f\"Epoch {epoch} validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for _ in val_bar:\n",
        "                n_way_val = random.choice([5, 10])\n",
        "                s_x, s_y, q_x, q_y = episodic_val.sample_episode(n_way_val, K_SHOT, Q_QUERY)\n",
        "                s_x = s_x.to(device); q_x = q_x.to(device)\n",
        "                s_y = s_y.to(device); q_y = q_y.to(device)\n",
        "                s_emb, scale = model(s_x)\n",
        "                q_emb, _ = model(q_x)\n",
        "                _, vacc = prototypical_loss_cosine(s_emb, s_y, q_emb, q_y, n_way_val, K_SHOT, scale)\n",
        "                val_accs.append(vacc)\n",
        "                val_bar.set_postfix({\"v_acc\": f\"{vacc*100:.2f}%\"})\n",
        "\n",
        "        mean_val = float(torch.tensor(val_accs).mean().item())\n",
        "        tqdm.write(f\"  val_acc (mean over {val_episodes} episodes) = {mean_val*100:.2f}%\")\n",
        "\n",
        "        # checkpoint best\n",
        "        if mean_val > best_val:\n",
        "            best_val = mean_val\n",
        "            ckpt = {\"epoch\": epoch, \"model_state\": model.state_dict(), \"opt_state\": opt.state_dict(), \"val_acc\": best_val}\n",
        "            torch.save(ckpt, os.path.join(CHECKPOINT_DIR, \"protonet_best.pth\"))\n",
        "            tqdm.write(f\"  --> New best val acc: {best_val*100:.2f}%  (checkpoint saved)\")\n",
        "\n",
        "        # Unfreeze after stage1 epochs\n",
        "        if epoch == STAGE1_EPOCHS:\n",
        "            tqdm.write(\"Switching to Stage 2: unfreezing backbone and creating new optimizer with param groups.\")\n",
        "            for p in model.features.parameters():\n",
        "                p.requires_grad = True\n",
        "            model.features.train()\n",
        "            opt = torch.optim.Adam([\n",
        "                {'params': model.features.parameters(), 'lr': LR_BACKBONE},\n",
        "                {'params': model.fc.parameters(), 'lr': LR_HEAD},\n",
        "                {'params': [model.log_scale], 'lr': LR_HEAD},\n",
        "            ], weight_decay=WEIGHT_DECAY)\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=12, gamma=0.5)\n",
        "\n",
        "    torch.save({\"model_state\": model.state_dict(), \"best_val\": best_val}, os.path.join(CHECKPOINT_DIR, \"protonet_final.pth\"))\n",
        "    tqdm.write(\"Training complete. Best val acc: {:.2f}%\".format(best_val*100))\n",
        "    return model\n",
        "\n",
        "# ----------------------------\n",
        "# Meta-test with progress bar\n",
        "# ----------------------------\n",
        "class EpisodicDatasetFromFolders:\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
        "        self.by_class = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "            folder = os.path.join(root, c)\n",
        "            imgs = [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            if len(imgs) >= 1:\n",
        "                self.by_class[i] = imgs\n",
        "        self.class_ids = list(self.by_class.keys())\n",
        "        if len(self.class_ids) == 0:\n",
        "            raise ValueError(\"No classes found in novel root: \" + root)\n",
        "\n",
        "    def sample_episode(self, n_way, k_shot, q_query):\n",
        "        chosen = random.sample(self.class_ids, n_way)\n",
        "        support_x, support_y, query_x, query_y = [], [], [], []\n",
        "        from PIL import Image\n",
        "        for i, cid in enumerate(chosen):\n",
        "            imgs = random.sample(self.by_class[cid], k_shot + q_query)\n",
        "            for si in imgs[:k_shot]:\n",
        "                im = Image.open(si).convert(\"RGB\")\n",
        "                support_x.append(self.transform(im))\n",
        "                support_y.append(i)\n",
        "            for qi in imgs[k_shot:]:\n",
        "                im = Image.open(qi).convert(\"RGB\")\n",
        "                query_x.append(self.transform(im))\n",
        "                query_y.append(i)\n",
        "        return torch.stack(support_x), torch.tensor(support_y), torch.stack(query_x), torch.tensor(query_y)\n",
        "\n",
        "def meta_test(model, novel_root):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "    ])\n",
        "    ds = EpisodicDatasetFromFolders(novel_root, transform=transform)\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    meta_bar = tqdm(range(META_TEST_EPISODES), desc=\"Meta-test episodes\")\n",
        "    with torch.no_grad():\n",
        "        for _ in meta_bar:\n",
        "            s_x, s_y, q_x, q_y = ds.sample_episode(META_TEST_N, META_TEST_K, META_TEST_Q)\n",
        "            s_x, q_x = s_x.to(device), q_x.to(device)\n",
        "            s_y, q_y = s_y.to(device), q_y.to(device)\n",
        "            s_emb, scale = model(s_x)\n",
        "            q_emb, _ = model(q_x)\n",
        "            _, acc = prototypical_loss_cosine(s_emb, s_y, q_emb, q_y, META_TEST_N, META_TEST_K, scale)\n",
        "            accs.append(acc)\n",
        "            meta_bar.set_postfix({\"acc\": f\"{acc*100:.2f}%\"})\n",
        "    accs = torch.tensor(accs)\n",
        "    mean = accs.mean().item()\n",
        "    std = accs.std().item()\n",
        "    ci95 = 1.96 * std / math.sqrt(len(accs))\n",
        "    tqdm.write(f\"Meta-test {META_TEST_N}-way {META_TEST_K}-shot: mean={mean*100:.2f}%, ±{ci95*100:.2f}% (95% CI)\")\n",
        "    return mean, ci95\n",
        "\n",
        "# ----------------------------\n",
        "# Run\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    model = train_protonet()\n",
        "    # Example meta-test usage:\n",
        "    # novel_root = \"/path/to/novel_classes_folder\"\n",
        "    # meta_test(model, novel_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qF2hqofUQprU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}