{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fc6b12",
   "metadata": {},
   "source": [
    "**Matting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956e4faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.24.1+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.8.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.3.0)\n",
      "Collecting pillow\n",
      "  Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.6)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting black>=24.10.0 (from kaggle)\n",
      "  Downloading black-25.11.0-cp313-cp313-win_amd64.whl.metadata (85 kB)\n",
      "Collecting bleach (from kaggle)\n",
      "  Downloading bleach-6.3.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting kagglesdk (from kaggle)\n",
      "  Downloading kagglesdk-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy>=1.15.0 (from kaggle)\n",
      "  Downloading mypy-1.18.2-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (6.33.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (70.2.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Collecting types-requests (from kaggle)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting types-tqdm (from kaggle)\n",
      "  Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting click>=8.0.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mypy-extensions>=0.4.3 (from black>=24.10.0->kaggle)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
      "Collecting pytokens>=0.3.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.6.0 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from mypy>=1.15.0->kaggle) (4.15.0)\n",
      "Collecting webencodings (from bleach->kaggle)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kaggle) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prabhakar\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->kaggle) (2025.11.12)\n",
      "Downloading kaggle-1.8.0-py3-none-any.whl (256 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 3.7/7.0 MB 18.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 17.9 MB/s  0:00:00\n",
      "Downloading matplotlib-3.10.7-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 4.5/8.1 MB 22.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 18.8 MB/s  0:00:00\n",
      "Downloading black-25.11.0-cp313-cp313-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 11.7 MB/s  0:00:00\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading mypy-1.18.2-cp313-cp313-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 3.7/9.8 MB 17.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.6/9.8 MB 15.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 12.5 MB/s  0:00:00\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Downloading kagglesdk-0.1.13-py3-none-any.whl (159 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl (24 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, text-unidecode, types-requests, pytokens, python-slugify, pillow, pathspec, mypy-extensions, click, bleach, types-tqdm, mypy, matplotlib, kagglesdk, black, kaggle\n",
      "\n",
      "   ----- ----------------------------------  2/16 [types-requests]\n",
      "  Attempting uninstall: pillow\n",
      "   ----- ----------------------------------  2/16 [types-requests]\n",
      "    Found existing installation: pillow 11.3.0\n",
      "   ----- ----------------------------------  2/16 [types-requests]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "    Uninstalling pillow-11.3.0:\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "      Successfully uninstalled pillow-11.3.0\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [pillow]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   -------------------- -------------------  8/16 [click]\n",
      "   ---------------------- -----------------  9/16 [bleach]\n",
      "   ---------------------- -----------------  9/16 [bleach]\n",
      "   ---------------------- -----------------  9/16 [bleach]\n",
      "   ------------------------- -------------- 10/16 [types-tqdm]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "  Attempting uninstall: matplotlib\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "    Found existing installation: matplotlib 3.10.6\n",
      "   --------------------------- ------------ 11/16 [mypy]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "    Uninstalling matplotlib-3.10.6:\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "      Successfully uninstalled matplotlib-3.10.6\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   ------------------------------ --------- 12/16 [matplotlib]\n",
      "   -------------------------------- ------- 13/16 [kagglesdk]\n",
      "   ----------------------------------- ---- 14/16 [black]\n",
      "   ------------------------------------- -- 15/16 [kaggle]\n",
      "   ------------------------------------- -- 15/16 [kaggle]\n",
      "   ------------------------------------- -- 15/16 [kaggle]\n",
      "   ---------------------------------------- 16/16 [kaggle]\n",
      "\n",
      "Successfully installed black-25.11.0 bleach-6.3.0 click-8.3.1 kaggle-1.8.0 kagglesdk-0.1.13 matplotlib-3.10.7 mypy-1.18.2 mypy-extensions-1.1.0 pathspec-0.12.1 pillow-12.0.0 python-slugify-8.0.4 pytokens-0.3.0 text-unidecode-1.3 types-requests-2.32.4.20250913 types-tqdm-4.67.0.20250809 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to install dependencies if missing.\n",
    "# You can skip if torch, torchvision and kaggle are installed and GPU-ready.\n",
    "\n",
    "!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -U kaggle pillow tqdm matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70a556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è GPU not found ‚Äî using CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"üöÄ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA runtime version:\", torch.version.cuda)\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è GPU not found ‚Äî using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d67314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKDIR: c:\\Users\\PRABHAKAR\\Downloads\\matting_project\n",
      "LOCAL_BG_DATASET_PATH: C:\\Users\\PRABHAKAR\\Downloads\\1\\BG-20k\n",
      "FG_DIR: c:\\Users\\PRABHAKAR\\Downloads\\matting_project\\fg\n",
      "ALPHA_DIR: c:\\Users\\PRABHAKAR\\Downloads\\matting_project\\alpha\n",
      "BG_OUT_DIR: c:\\Users\\PRABHAKAR\\Downloads\\matting_project\\bg\n",
      "IMG_SIZE: 320 BATCH_SIZE: 8\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ---------------- CONFIG - EDIT THESE ----------------\n",
    "WORKDIR = Path.cwd() / \"matting_project\"    # main workspace\n",
    "LOCAL_BG_DATASET_PATH = r'C:\\Users\\PRABHAKAR\\Downloads\\1\\BG-20k'  # <-- set this to your local dataset root\n",
    "NUM_BG_TO_COPY = 5000        # how many backgrounds to copy/use (or smaller)\n",
    "IMG_SIZE = 320\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 30\n",
    "LR = 1e-4\n",
    "SAVE_DIR = WORKDIR / \"checkpoints\"\n",
    "FG_DIR = WORKDIR / \"fg\"        # put your foreground images here\n",
    "ALPHA_DIR = WORKDIR / \"alpha\"  # put your alpha mattes here\n",
    "BG_OUT_DIR = WORKDIR / \"bg\"    # backgrounds will be copied here\n",
    "# ----------------------------------------------------\n",
    "\n",
    "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ALPHA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BG_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"WORKDIR:\", WORKDIR)\n",
    "print(\"LOCAL_BG_DATASET_PATH:\", LOCAL_BG_DATASET_PATH)\n",
    "print(\"FG_DIR:\", FG_DIR)\n",
    "print(\"ALPHA_DIR:\", ALPHA_DIR)\n",
    "print(\"BG_OUT_DIR:\", BG_OUT_DIR)\n",
    "print(\"IMG_SIZE:\", IMG_SIZE, \"BATCH_SIZE:\", BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2caaf701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected background source directory: C:\\Users\\PRABHAKAR\\Downloads\\1\\BG-20k\\train\n",
      "Copied 1682 images to c:\\Users\\PRABHAKAR\\Downloads\\matting_project\\bg\n"
     ]
    }
   ],
   "source": [
    "# This cell will search LOCAL_BG_DATASET_PATH for 'train' first, then 'test', else any folder with images.\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "root = Path(LOCAL_BG_DATASET_PATH)\n",
    "assert root.exists(), f\"LOCAL_BG_DATASET_PATH not found: {root}\"\n",
    "\n",
    "# Candidate target subfolders (prefer 'train', then 'test')\n",
    "candidates = []\n",
    "train_dir = root / \"train\"\n",
    "test_dir = root / \"test\"\n",
    "\n",
    "if train_dir.exists():\n",
    "    candidates.append(train_dir)\n",
    "if test_dir.exists():\n",
    "    candidates.append(test_dir)\n",
    "\n",
    "# If neither exists, find first folder that contains images (recursively)\n",
    "if not candidates:\n",
    "    for p in root.iterdir():\n",
    "        # check if this entry or its subdirs contain image files\n",
    "        found = False\n",
    "        for ext in (\".jpg\", \".jpeg\", \".png\"):\n",
    "            if any(p.rglob(f\"*{ext}\")):\n",
    "                found = True\n",
    "                break\n",
    "        if found and p.is_dir():\n",
    "            candidates.append(p)\n",
    "            break\n",
    "\n",
    "if not candidates:\n",
    "    # fallback: search whole tree and pick parent of first image found\n",
    "    first_img_parent = None\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]:\n",
    "            first_img_parent = p.parent\n",
    "            break\n",
    "    if first_img_parent:\n",
    "        candidates.append(first_img_parent)\n",
    "\n",
    "assert candidates, f\"No background images found under {root}. Check your dataset structure.\"\n",
    "\n",
    "selected_bg_dir = Path(candidates[0])\n",
    "print(\"Selected background source directory:\", selected_bg_dir)\n",
    "\n",
    "# Copy up to NUM_BG_TO_COPY images into BG_OUT_DIR\n",
    "copied = 0\n",
    "for img_path in sorted(selected_bg_dir.glob(\"*\")):\n",
    "    if img_path.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "        continue\n",
    "    dst = BG_OUT_DIR / img_path.name\n",
    "    if not dst.exists():\n",
    "        shutil.copy(img_path, dst)\n",
    "        copied += 1\n",
    "    if copied >= NUM_BG_TO_COPY:\n",
    "        break\n",
    "\n",
    "print(f\"Copied {copied} images to {BG_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0aee74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PRABHAKAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random, math, time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7cd9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== REDEFINED Dataset class (use PIL.rotate to avoid torchvision API mismatch) =====\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MattingCompositeDataset(Dataset):\n",
    "    def __init__(self, fg_list, alpha_list, bg_dir, img_size=320, augment=True):\n",
    "        self.fg_files = fg_list\n",
    "        self.alpha_files = alpha_list\n",
    "        self.bg_files = sorted([p for p in Path(bg_dir).glob(\"*\") if p.suffix.lower() in [\".png\",\".jpg\",\".jpeg\"]])\n",
    "        assert len(self.fg_files) == len(self.alpha_files), \"FG and ALPHA counts must match\"\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def open_img(self, path, mode=\"RGB\"):\n",
    "        return Image.open(path).convert(mode)\n",
    "\n",
    "    def resize_and_center_crop(self, img):\n",
    "        w, h = img.size\n",
    "        scale = max(self.img_size / w, self.img_size / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        # use BILINEAR for RGB and NEAREST for masks\n",
    "        resample = Image.BILINEAR if img.mode == \"RGB\" else Image.NEAREST\n",
    "        img = img.resize((new_w, new_h), resample=resample)\n",
    "        left = (new_w - self.img_size)//2\n",
    "        top = (new_h - self.img_size)//2\n",
    "        img = img.crop((left, top, left + self.img_size, top + self.img_size))\n",
    "        return img\n",
    "\n",
    "    def random_transform(self, fg, alpha, bg):\n",
    "        # Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            fg = fg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            alpha = alpha.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            bg = bg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        # Slight random rotation using PIL.Image.rotate (resample arg supported)\n",
    "        if random.random() < 0.3:\n",
    "            angle = random.uniform(-10, 10)\n",
    "            fg = fg.rotate(angle, resample=Image.BILINEAR)\n",
    "            alpha = alpha.rotate(angle, resample=Image.NEAREST)\n",
    "            bg = bg.rotate(angle, resample=Image.BILINEAR)\n",
    "        return fg, alpha, bg\n",
    "\n",
    "    def to_tensor(self, img):\n",
    "        arr = np.array(img).astype(np.float32) / 255.0\n",
    "        if img.mode == \"L\":\n",
    "            arr = np.expand_dims(arr, axis=2)\n",
    "        arr = np.transpose(arr, (2,0,1))\n",
    "        return torch.from_numpy(arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fg_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fg = self.open_img(self.fg_files[idx], mode=\"RGB\")\n",
    "        alpha = self.open_img(self.alpha_files[idx], mode=\"L\")\n",
    "        bg = self.open_img(random.choice(self.bg_files), mode=\"RGB\")\n",
    "\n",
    "        fg = self.resize_and_center_crop(fg)\n",
    "        alpha = self.resize_and_center_crop(alpha)\n",
    "        bg = self.resize_and_center_crop(bg)\n",
    "\n",
    "        if self.augment:\n",
    "            fg, alpha, bg = self.random_transform(fg, alpha, bg)\n",
    "\n",
    "        fg_t = self.to_tensor(fg)\n",
    "        alpha_t = self.to_tensor(alpha)\n",
    "        bg_t = self.to_tensor(bg)\n",
    "        comp_t = fg_t * alpha_t + bg_t * (1 - alpha_t)\n",
    "\n",
    "        return {\"comp\": comp_t.float(), \"bg\": bg_t.float(), \"alpha\": alpha_t.float(), \"fg\": fg_t.float()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02916f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataloaders created.\n",
      " - Train samples: 900\n",
      " - Val samples:   100\n",
      " - BG images:     1682\n",
      "\n",
      "Sample batch keys: ['comp', 'bg', 'alpha', 'fg']\n",
      "Shapes:\n",
      " comp: torch.Size([8, 3, 320, 320])\n",
      " bg:   torch.Size([8, 3, 320, 320])\n",
      " alpha: torch.Size([8, 1, 320, 320])\n",
      " fg:   torch.Size([8, 3, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "# === RECREATE DATASETS & DATALOADERS (run this now) ===\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# If train_fg / train_alpha / val_fg / val_alpha are not in memory, re-run the pairing cell first.\n",
    "try:\n",
    "    _ = train_fg, train_alpha, val_fg, val_alpha\n",
    "except NameError:\n",
    "    raise RuntimeError(\"train_fg / train_alpha / val_fg / val_alpha not found. Re-run the pairing cell (robust pairing) first.\")\n",
    "\n",
    "# Create dataset instances using the newly defined MattingCompositeDataset\n",
    "train_ds = MattingCompositeDataset(train_fg, train_alpha, BG_OUT_DIR, img_size=IMG_SIZE, augment=True)\n",
    "val_ds   = MattingCompositeDataset(val_fg, val_alpha, BG_OUT_DIR, img_size=IMG_SIZE, augment=False)\n",
    "\n",
    "# DataLoader workers: Windows -> use 0, otherwise 4\n",
    "num_workers = 4\n",
    "if os.name == \"nt\":\n",
    "    num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=max(0, num_workers//2), pin_memory=True)\n",
    "\n",
    "print(\"‚úÖ Dataloaders created.\")\n",
    "print(\" - Train samples:\", len(train_ds))\n",
    "print(\" - Val samples:  \", len(val_ds))\n",
    "print(\" - BG images:    \", len(list(BG_OUT_DIR.glob('*'))))\n",
    "\n",
    "# show a quick batch to ensure dataloader works\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nSample batch keys:\", list(batch.keys()))\n",
    "print(\"Shapes:\")\n",
    "print(\" comp:\", batch['comp'].shape)\n",
    "print(\" bg:  \", batch['bg'].shape)\n",
    "print(\" alpha:\", batch['alpha'].shape)\n",
    "print(\" fg:  \", batch['fg'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10645323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNetMatte(nn.Module):\n",
    "    def __init__(self, in_ch=4, base_ch=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch*2)\n",
    "        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n",
    "        self.center = ConvBlock(base_ch*4, base_ch*8)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(base_ch*8, base_ch*4)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base_ch*4, base_ch*2)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(base_ch*2, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        c = self.center(self.pool(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(c), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        out = torch.sigmoid(self.out_conv(d1))\n",
    "        return out\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, out_channels=3, pretrained_resnet=None):\n",
    "        super().__init__()\n",
    "        if pretrained_resnet is None:\n",
    "            pretrained_resnet = models.resnet34(pretrained=True)\n",
    "        # Encoder parts from pretrained resnet34\n",
    "        # Note: conv1 -> bn1 -> relu produces x0 with 64 channels\n",
    "        self.inc_conv = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # copy pretrained weights for RGB channels, zero-init mask channel if n_channels==4\n",
    "        w = pretrained_resnet.conv1.weight.data.clone()\n",
    "        self.inc_conv.weight.data[:, :3, :, :] = w\n",
    "        if n_channels == 4:\n",
    "            self.inc_conv.weight.data[:, 3:, :, :].zero_()\n",
    "        self.inc_bn = pretrained_resnet.bn1\n",
    "        self.inc_relu = pretrained_resnet.relu\n",
    "        self.maxpool = pretrained_resnet.maxpool\n",
    "\n",
    "        self.enc1 = pretrained_resnet.layer1   # 64 channels\n",
    "        self.enc2 = pretrained_resnet.layer2   # 128 channels\n",
    "        self.enc3 = pretrained_resnet.layer3   # 256 channels\n",
    "        self.enc4 = pretrained_resnet.layer4   # 512 channels\n",
    "\n",
    "        # decoder conv blocks (decoders take concatenated inputs)\n",
    "        def dec_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        # upsample transpose convs to bring spatial resolution up\n",
    "        self.up_transpose0 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)  # e4 -> 256\n",
    "        self.dec4 = dec_block(256 + 256, 256)   # concat with e3 (256) -> input 512 -> out 256\n",
    "\n",
    "        self.up_transpose1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 256 -> 128\n",
    "        self.dec3 = dec_block(128 + 128, 128)   # concat with e2 (128)\n",
    "\n",
    "        self.up_transpose2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)   # 128 -> 64\n",
    "        self.dec2 = dec_block(64 + 64, 64)      # concat with e1 (64)\n",
    "\n",
    "        self.up_transpose3 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)    # 64 -> 64 (upsample)\n",
    "        self.dec1 = dec_block(64 + 64, 64)      # concat with x0 (64)\n",
    "\n",
    "        self.outconv = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.inc_conv(x)       # [B,64,H/2,W/2]\n",
    "        x0 = self.inc_bn(x0)\n",
    "        x0 = self.inc_relu(x0)\n",
    "\n",
    "        x1 = self.maxpool(x0)       # [B,64,H/4,W/4]\n",
    "        e1 = self.enc1(x1)          # [B,64,H/4,W/4]\n",
    "        e2 = self.enc2(e1)          # [B,128,H/8,W/8]\n",
    "        e3 = self.enc3(e2)          # [B,256,H/16,W/16]\n",
    "        e4 = self.enc4(e3)          # [B,512,H/32,W/32]\n",
    "\n",
    "        # Decoder: upsample -> concat with corresponding encoder -> conv block\n",
    "        u4 = self.up_transpose0(e4)           # [B,256,H/16,W/16]\n",
    "        u4 = torch.cat([u4, e3], dim=1)       # [B,512,H/16,W/16]\n",
    "        u4 = self.dec4(u4)                    # [B,256,H/16,W/16]\n",
    "\n",
    "        u3 = self.up_transpose1(u4)           # [B,128,H/8,W/8]\n",
    "        u3 = torch.cat([u3, e2], dim=1)       # [B,256,H/8,W/8]\n",
    "        u3 = self.dec3(u3)                    # [B,128,H/8,W/8]\n",
    "\n",
    "        u2 = self.up_transpose2(u3)           # [B,64,H/4,W/4]\n",
    "        u2 = torch.cat([u2, e1], dim=1)       # [B,128,H/4,W/4]\n",
    "        u2 = self.dec2(u2)                    # [B,64,H/4,W/4]\n",
    "\n",
    "        u1 = self.up_transpose3(u2)           # [B,64,H/2,W/2]\n",
    "        # Note: x0 has shape [B,64,H/2,W/2], so concat is valid\n",
    "        u1 = torch.cat([u1, x0], dim=1)       # [B,128,H/2,W/2]\n",
    "        u1 = self.dec1(u1)                    # [B,64,H/2,W/2]\n",
    "\n",
    "        out = self.outconv(u1)                # [B,3,H/2,W/2] -> but we need full resolution\n",
    "        # If needed, upsample to original input size\n",
    "        out = F.interpolate(out, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9db9ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_composition_loss(pred_alpha, true_alpha, fg, bg):\n",
    "    l_alpha = torch.mean(torch.abs(pred_alpha - true_alpha))\n",
    "    comp_pred = fg * pred_alpha + bg * (1 - pred_alpha)\n",
    "    comp_true = fg * true_alpha + bg * (1 - true_alpha)\n",
    "    l_comp = torch.mean(torch.abs(comp_pred - comp_true))\n",
    "    return l_alpha + l_comp, l_alpha, l_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88ad7496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images ‚Äî creating FG + coarse ALPHA (saving to matting_project\\fg and matting_project\\alpha)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# optional resize when saving to project (set IMG_OUT_SIZE to (W,H) if needed)\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m IMG_OUT_SIZE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PRABHAKAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\Image.py:972\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    921\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    922\u001b[39m     mode: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    926\u001b[39m     colors: \u001b[38;5;28mint\u001b[39m = \u001b[32m256\u001b[39m,\n\u001b[32m    927\u001b[39m ) -> Image:\n\u001b[32m    928\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[33;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m \u001b[33;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PRABHAKAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\ImageFile.py:394\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    391\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    393\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Auto-generate coarse alpha masks from images (for debugging/training) ===\n",
    "# Paste this as one cell and run.\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil, os, sys\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "# ---- EDIT this if your source images are elsewhere ----\n",
    "SRC_IMG_FOLDER = r\"C:\\Users\\PRABHAKAR\\Downloads\\1\"   # default candidate you found\n",
    "MAX_SAMPLES = 1000    # how many images to process (reduce if needed)\n",
    "IMG_OUT_SIZE = None   # optionally resize when saving FG (None = keep original crop in later steps)\n",
    "\n",
    "# ---- destinations created by notebook config (DO NOT change unless you know what you're doing) ----\n",
    "FG_OUT = Path(\"matting_project\") / \"fg\"\n",
    "ALPHA_OUT = Path(\"matting_project\") / \"alpha\"\n",
    "FG_OUT.mkdir(parents=True, exist_ok=True)\n",
    "ALPHA_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- ensure opencv available; if not, install it ----\n",
    "try:\n",
    "    import cv2\n",
    "except Exception:\n",
    "    print(\"opencv not found ‚Äî installing opencv-python (this may take a minute)...\")\n",
    "    !pip install -q opencv-python\n",
    "    import importlib\n",
    "    importlib.reload(sys.modules.get('cv2', None) or __import__('cv2'))\n",
    "    import cv2\n",
    "\n",
    "# ---- gather image files ----\n",
    "src = Path(SRC_IMG_FOLDER)\n",
    "if not src.exists():\n",
    "    raise RuntimeError(f\"SRC folder not found: {src}\\nChange SRC_IMG_FOLDER to the folder that contains your images.\")\n",
    "\n",
    "imgs = sorted([p for p in src.rglob(\"*\") if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]])[:MAX_SAMPLES]\n",
    "if len(imgs) == 0:\n",
    "    raise RuntimeError(f\"No images found under {src}\")\n",
    "\n",
    "print(f\"Found {len(imgs)} images ‚Äî creating FG + coarse ALPHA (saving to {FG_OUT} and {ALPHA_OUT})\")\n",
    "\n",
    "def make_coarse_alpha(pil_img):\n",
    "    \"\"\"Return a uint8 mask (0..255) estimated from a PIL RGB image.\"\"\"\n",
    "    # Convert to grayscale array\n",
    "    gray = np.array(pil_img.convert(\"L\"))\n",
    "    # Blur to reduce noise\n",
    "    blur = cv2.GaussianBlur(gray, (7,7), 0)\n",
    "    # Otsu threshold\n",
    "    _, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # Morphological closing then opening to reduce holes/noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    # Heuristic: if mask covers >75% of area then invert (likely background chosen)\n",
    "    if (mask > 0).mean() > 0.75:\n",
    "        mask = 255 - mask\n",
    "    return mask\n",
    "\n",
    "copied = 0\n",
    "for p in imgs:\n",
    "    try:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        # optional resize when saving to project (set IMG_OUT_SIZE to (W,H) if needed)\n",
    "        if IMG_OUT_SIZE:\n",
    "            img_save = img.resize(IMG_OUT_SIZE, Image.BILINEAR)\n",
    "        else:\n",
    "            img_save = img\n",
    "        # create mask\n",
    "        mask_arr = make_coarse_alpha(img)\n",
    "        # make alpha file name\n",
    "        fg_name = FG_OUT / p.name\n",
    "        alpha_name = ALPHA_OUT / (p.stem + \"_alpha.png\")\n",
    "        # save\n",
    "        img_save.save(fg_name)\n",
    "        Image.fromarray(mask_arr).save(alpha_name)\n",
    "        copied += 1\n",
    "    except Exception as e:\n",
    "        print(\"skip\", p, \":\", e)\n",
    "print(f\"Done. Saved {copied} FG images and {copied} ALPHA masks.\")\n",
    "\n",
    "# show a few samples\n",
    "fg_samples = sorted([x.name for x in FG_OUT.glob(\"*\")])[:10]\n",
    "alpha_samples = sorted([x.name for x in ALPHA_OUT.glob(\"*\")])[:10]\n",
    "print(\"Sample FG files:\", fg_samples)\n",
    "print(\"Sample ALPHA files:\", alpha_samples)\n",
    "\n",
    "# quick visualize 3 random examples inline (if in notebook)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from random import sample\n",
    "    picks = sample(imgs, min(3, len(imgs)))\n",
    "    fig, axs = plt.subplots(3,3, figsize=(12,12))\n",
    "    for i,p in enumerate(picks):\n",
    "        fg_p = FG_OUT / p.name\n",
    "        alpha_p = ALPHA_OUT / (p.stem + \"_alpha.png\")\n",
    "        fg_im = np.array(Image.open(fg_p).convert(\"RGB\"))\n",
    "        alpha_im = np.array(Image.open(alpha_p).convert(\"L\"))\n",
    "        axs[i,0].imshow(fg_im); axs[i,0].set_title(\"FG\"); axs[i,0].axis('off')\n",
    "        axs[i,1].imshow(alpha_im, cmap='gray'); axs[i,1].set_title(\"Alpha\"); axs[i,1].axis('off')\n",
    "        # composite with a gray background for display\n",
    "        bg = np.full_like(fg_im, 128)\n",
    "        alpha_norm = alpha_im[...,None].astype(np.float32)/255.0\n",
    "        comp = (fg_im.astype(np.float32)*alpha_norm + bg.astype(np.float32)*(1-alpha_norm)).astype(np.uint8)\n",
    "        axs[i,2].imshow(comp); axs[i,2].set_title(\"Composite\"); axs[i,2].axis('off')\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\nNext: run the Fixed Cell 9 (the pairing + train/val creation cell) to build train/val loaders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found FG images: 1000\n",
      "Found ALPHA images: 1000\n",
      "Pairs after exact cleaned-stem matching: 1000\n",
      "Total final matched pairs: 1000\n",
      "\n",
      "Sample matched pairs (first 20):\n",
      " 1. FG: h_0006a281.jpg   <--->   ALPHA: h_0006a281_alpha.png\n",
      " 2. FG: h_00241c18.jpg   <--->   ALPHA: h_00241c18_alpha.png\n",
      " 3. FG: h_00577a09.jpg   <--->   ALPHA: h_00577a09_alpha.png\n",
      " 4. FG: h_00610cbc.jpg   <--->   ALPHA: h_00610cbc_alpha.png\n",
      " 5. FG: h_00d476bd.jpg   <--->   ALPHA: h_00d476bd_alpha.png\n",
      " 6. FG: h_0100ce66.jpg   <--->   ALPHA: h_0100ce66_alpha.png\n",
      " 7. FG: h_010380c8.jpg   <--->   ALPHA: h_010380c8_alpha.png\n",
      " 8. FG: h_016722ee.jpg   <--->   ALPHA: h_016722ee_alpha.png\n",
      " 9. FG: h_019af812.jpg   <--->   ALPHA: h_019af812_alpha.png\n",
      "10. FG: h_01a531f7.jpg   <--->   ALPHA: h_01a531f7_alpha.png\n",
      "11. FG: h_01d3e6f5.jpg   <--->   ALPHA: h_01d3e6f5_alpha.png\n",
      "12. FG: h_01d636e5.jpg   <--->   ALPHA: h_01d636e5_alpha.png\n",
      "13. FG: h_01e42270.jpg   <--->   ALPHA: h_01e42270_alpha.png\n",
      "14. FG: h_01ed5c4a.jpg   <--->   ALPHA: h_01ed5c4a_alpha.png\n",
      "15. FG: h_02007da7.jpg   <--->   ALPHA: h_02007da7_alpha.png\n",
      "16. FG: h_022ed959.jpg   <--->   ALPHA: h_022ed959_alpha.png\n",
      "17. FG: h_023e2c2a.jpg   <--->   ALPHA: h_023e2c2a_alpha.png\n",
      "18. FG: h_023e2cbf.jpg   <--->   ALPHA: h_023e2cbf_alpha.png\n",
      "19. FG: h_0266fbdb.jpg   <--->   ALPHA: h_0266fbdb_alpha.png\n",
      "20. FG: h_026f03f6.jpg   <--->   ALPHA: h_026f03f6_alpha.png\n",
      "\n",
      "Unmatched FG count: 0  (sample: [])\n",
      "Unmatched ALPHA count: 0  (sample: [])\n",
      "\n",
      "Train pairs: 900  Val pairs: 100\n",
      "\n",
      "‚úî DATA READY:\n",
      " - Train samples: 900\n",
      " - Val samples: 100\n",
      " - Background images: 1682\n"
     ]
    }
   ],
   "source": [
    "# ======= Robust Cell 9: Pair FG & ALPHA with multiple heuristics and create dataloaders =======\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "# gather files\n",
    "fg_files_raw = sorted([p for p in FG_DIR.glob(\"*\") if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]])\n",
    "alpha_files_raw = sorted([p for p in ALPHA_DIR.glob(\"*\") if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]])\n",
    "\n",
    "print(\"Found FG images:\", len(fg_files_raw))\n",
    "print(\"Found ALPHA images:\", len(alpha_files_raw))\n",
    "\n",
    "if len(fg_files_raw) == 0:\n",
    "    raise RuntimeError(f\"No foreground images found in FG_DIR: {FG_DIR}\")\n",
    "if len(alpha_files_raw) == 0:\n",
    "    raise RuntimeError(f\"No alpha images found in ALPHA_DIR: {ALPHA_DIR}\")\n",
    "\n",
    "def clean_stem(p):\n",
    "    s = p.stem.lower()\n",
    "    s = re.sub(r'[^a-z0-9]', '', s)     # remove non-alphanum\n",
    "    s = s.replace(\"alpha\", \"\").replace(\"mask\",\"\").replace(\"matte\",\"\").replace(\"trimap\",\"\")\n",
    "    return s\n",
    "\n",
    "# maps\n",
    "fg_map = {clean_stem(p): p for p in fg_files_raw}\n",
    "alpha_map = {clean_stem(p): p for p in alpha_files_raw}\n",
    "\n",
    "paired_fg = []\n",
    "paired_alpha = []\n",
    "used_alpha_keys = set()\n",
    "\n",
    "# Strategy 1: exact cleaned-stem match\n",
    "for k, pfg in fg_map.items():\n",
    "    if k in alpha_map:\n",
    "        paired_fg.append(pfg)\n",
    "        paired_alpha.append(alpha_map[k])\n",
    "        used_alpha_keys.add(k)\n",
    "\n",
    "print(\"Pairs after exact cleaned-stem matching:\", len(paired_fg))\n",
    "\n",
    "# Strategy 2: if nothing matched and counts equal -> pair by sorted order\n",
    "if len(paired_fg) == 0 and len(fg_files_raw) == len(alpha_files_raw):\n",
    "    print(\"No exact matches ‚Äî counts equal. Pairing by sorted order (fallback).\")\n",
    "    for f, a in zip(sorted(fg_files_raw), sorted(alpha_files_raw)):\n",
    "        paired_fg.append(f); paired_alpha.append(a)\n",
    "    used_alpha_keys = set([clean_stem(a) for a in paired_alpha])\n",
    "\n",
    "# Strategy 3: numeric ID matching (extract first number sequence from filename)\n",
    "if len(paired_fg) == 0:\n",
    "    print(\"Trying numeric-ID matching...\")\n",
    "    def extract_num(p):\n",
    "        m = re.search(r'(\\d{1,8})', p.stem)\n",
    "        return m.group(1) if m else None\n",
    "    num_to_alpha = defaultdict(list)\n",
    "    for a in alpha_files_raw:\n",
    "        num = extract_num(a)\n",
    "        if num:\n",
    "            num_to_alpha[num].append(a)\n",
    "    for f in fg_files_raw:\n",
    "        num = extract_num(f)\n",
    "        if num and num in num_to_alpha and len(num_to_alpha[num])>0:\n",
    "            a = num_to_alpha[num].pop(0)\n",
    "            paired_fg.append(f); paired_alpha.append(a)\n",
    "            used_alpha_keys.add(clean_stem(a))\n",
    "    print(\"Pairs after numeric matching:\", len(paired_fg))\n",
    "\n",
    "# Strategy 4: fuzzy matching (difflib) for remaining unmatched fg\n",
    "if len(paired_fg) == 0 or len(paired_fg) < len(fg_files_raw):\n",
    "    print(\"Applying fuzzy matching for remaining items...\")\n",
    "    # build reverse maps keyed by original stem (not cleaned) for fuzzy compare\n",
    "    alpha_stems = [a.stem for a in alpha_files_raw]\n",
    "    alpha_stem_to_p = {a.stem: a for a in alpha_files_raw}\n",
    "    for f in fg_files_raw:\n",
    "        if f in paired_fg:\n",
    "            continue\n",
    "        # try matching using raw stem and cleaned stem\n",
    "        candidates = difflib.get_close_matches(f.stem, alpha_stems, n=3, cutoff=0.6)\n",
    "        if not candidates:\n",
    "            candidates = difflib.get_close_matches(clean_stem(f), list(alpha_map.keys()), n=3, cutoff=0.6)\n",
    "        matched = False\n",
    "        for c in candidates:\n",
    "            a_path = alpha_stem_to_p.get(c) or alpha_map.get(clean_stem(Path(c)))\n",
    "            if a_path and clean_stem(a_path) not in used_alpha_keys:\n",
    "                paired_fg.append(f); paired_alpha.append(a_path)\n",
    "                used_alpha_keys.add(clean_stem(a_path))\n",
    "                matched = True\n",
    "                break\n",
    "    print(\"Pairs after fuzzy matching:\", len(paired_fg))\n",
    "\n",
    "# Final: prune duplicates and ensure same length\n",
    "final_pairs = []\n",
    "seen_alpha = set()\n",
    "for f,p in zip(paired_fg, paired_alpha):\n",
    "    key = p.name\n",
    "    if key in seen_alpha:\n",
    "        continue\n",
    "    final_pairs.append((f,p))\n",
    "    seen_alpha.add(key)\n",
    "\n",
    "paired_fg = [x[0] for x in final_pairs]\n",
    "paired_alpha = [x[1] for x in final_pairs]\n",
    "\n",
    "print(\"Total final matched pairs:\", len(paired_fg))\n",
    "\n",
    "# Show examples of matches and unmatched\n",
    "def sample_list(lst, n=10):\n",
    "    return [p.name for p in lst[:n]]\n",
    "\n",
    "print(\"\\nSample matched pairs (first 20):\")\n",
    "for i,(f,a) in enumerate(zip(paired_fg[:20], paired_alpha[:20])):\n",
    "    print(f\"{i+1:2d}. FG: {f.name}   <--->   ALPHA: {a.name}\")\n",
    "\n",
    "unmatched_fg = [p for p in fg_files_raw if p not in paired_fg]\n",
    "unmatched_alpha = [p for p in alpha_files_raw if p not in paired_alpha]\n",
    "\n",
    "print(f\"\\nUnmatched FG count: {len(unmatched_fg)}  (sample: {sample_list(unmatched_fg,10)})\")\n",
    "print(f\"Unmatched ALPHA count: {len(unmatched_alpha)}  (sample: {sample_list(unmatched_alpha,10)})\")\n",
    "\n",
    "# If we still have zero pairs -> ask for samples\n",
    "if len(paired_fg) == 0:\n",
    "    raise RuntimeError(\"‚ùå No FG‚ÄìALPHA pairs could be formed with heuristics. Please paste 5 FG filenames and 5 ALPHA filenames (from the samples above) so I can create a custom matching rule.\")\n",
    "\n",
    "# Proceed: train/val split using matched pairs\n",
    "idxs = list(range(len(paired_fg)))\n",
    "train_idx, val_idx = train_test_split(idxs, test_size=0.1, random_state=42)\n",
    "\n",
    "train_fg = [paired_fg[i] for i in train_idx]\n",
    "train_alpha = [paired_alpha[i] for i in train_idx]\n",
    "val_fg = [paired_fg[i] for i in val_idx]\n",
    "val_alpha = [paired_alpha[i] for i in val_idx]\n",
    "\n",
    "print(\"\\nTrain pairs:\", len(train_fg), \" Val pairs:\", len(val_fg))\n",
    "\n",
    "# Create datasets & dataloaders\n",
    "train_ds = MattingCompositeDataset(train_fg, train_alpha, BG_OUT_DIR, img_size=IMG_SIZE, augment=True)\n",
    "val_ds = MattingCompositeDataset(val_fg, val_alpha, BG_OUT_DIR, img_size=IMG_SIZE, augment=False)\n",
    "\n",
    "num_workers = 4\n",
    "import os\n",
    "if os.name == \"nt\":\n",
    "    num_workers = 0\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=max(0, num_workers//2), pin_memory=True)\n",
    "\n",
    "print(\"\\n‚úî DATA READY:\")\n",
    "print(\" - Train samples:\", len(train_ds))\n",
    "print(\" - Val samples:\", len(val_ds))\n",
    "print(\" - Background images:\", len(list(BG_OUT_DIR.glob('*'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "115372a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 1928705\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "model = UNetMatte(in_ch=4, base_ch=32).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "165d91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prepare_input(batch):\n",
    "    comp = batch['comp'].to(DEVICE, non_blocking=True)\n",
    "    bg = batch['bg'].to(DEVICE, non_blocking=True)\n",
    "    alpha = batch['alpha'].to(DEVICE, non_blocking=True)\n",
    "    fg = batch['fg'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "    bg_gray = bg.mean(dim=1, keepdim=True)\n",
    "    inp = torch.cat([comp, bg_gray], dim=1)  # shape: (B,4,H,W)\n",
    "    return inp, comp, bg, alpha, fg\n",
    "\n",
    "def alpha_composition_loss(pred_alpha, true_alpha, fg, bg):\n",
    "    l_alpha = torch.mean(torch.abs(pred_alpha - true_alpha))\n",
    "\n",
    "    comp_pred = fg * pred_alpha + bg * (1 - pred_alpha)\n",
    "    comp_true = fg * true_alpha + bg * (1 - true_alpha)\n",
    "\n",
    "    l_comp = torch.mean(torch.abs(comp_pred - comp_true))\n",
    "    return l_alpha + l_comp, l_alpha, l_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73bb08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    running = defaultdict(float)\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        inp, comp, bg, alpha, fg = prepare_input(batch)\n",
    "        pred_alpha = model(inp)\n",
    "\n",
    "        loss, l_alpha, l_comp = alpha_composition_loss(pred_alpha, alpha, fg, bg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running['loss'] += loss.item()\n",
    "        running['l_alpha'] += l_alpha.item()\n",
    "        running['l_comp'] += l_comp.item()\n",
    "\n",
    "        pbar.set_postfix({k: f\"{running[k]/(pbar.n+1):.4f}\" for k in running})\n",
    "\n",
    "    for k in running:\n",
    "        running[k] /= len(loader)\n",
    "\n",
    "    return running\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    running = defaultdict(float)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for batch in pbar:\n",
    "            inp, comp, bg, alpha, fg = prepare_input(batch)\n",
    "            pred_alpha = model(inp)\n",
    "\n",
    "            loss, l_alpha, l_comp = alpha_composition_loss(pred_alpha, alpha, fg, bg)\n",
    "\n",
    "            running['loss'] += loss.item()\n",
    "            running['l_alpha'] += l_alpha.item()\n",
    "            running['l_comp'] += l_comp.item()\n",
    "\n",
    "            pbar.set_postfix({k: f\"{running[k]/(pbar.n+1):.4f}\" for k in running})\n",
    "\n",
    "        for k in running:\n",
    "            running[k] /= len(loader)\n",
    "\n",
    "    return running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11cae7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_sample(batch, pred_alpha=None, idx=0):\n",
    "    comp = batch['comp'][idx].cpu().numpy().transpose(1,2,0)\n",
    "    bg = batch['bg'][idx].cpu().numpy().transpose(1,2,0)\n",
    "    fg = batch['fg'][idx].cpu().numpy().transpose(1,2,0)\n",
    "    alpha = batch['alpha'][idx].cpu().numpy().transpose(1,2,0)[...,0]\n",
    "\n",
    "    if pred_alpha is not None:\n",
    "        pred = pred_alpha[idx].cpu().numpy().transpose(1,2,0)[...,0]\n",
    "    else:\n",
    "        pred = None\n",
    "\n",
    "    cols = 5 if pred is not None else 4\n",
    "    fig, ax = plt.subplots(1, cols, figsize=(4*cols,4))\n",
    "\n",
    "    ax[0].imshow(np.clip(comp,0,1)); ax[0].set_title(\"Composite\"); ax[0].axis('off')\n",
    "    ax[1].imshow(np.clip(bg,0,1)); ax[1].set_title(\"BG\"); ax[1].axis('off')\n",
    "    ax[2].imshow(np.clip(fg,0,1)); ax[2].set_title(\"FG\"); ax[2].axis('off')\n",
    "    ax[3].imshow(alpha, cmap='gray'); ax[3].set_title(\"True Alpha\"); ax[3].axis('off')\n",
    "\n",
    "    if pred is not None:\n",
    "        ax[4].imshow(pred, cmap='gray'); ax[4].set_title(\"Pred Alpha\"); ax[4].axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a38a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRABHAKAR\\AppData\\Local\\Temp\\ipykernel_10036\\1219489951.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= EPOCH 1/10 =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRABHAKAR\\AppData\\Local\\Temp\\ipykernel_10036\\1219489951.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 1: time=4.610s  loss=0.3539  iou=0.6569\n",
      " Batch 2: time=3.995s  loss=0.2786  iou=0.8504\n",
      " Batch 3: time=3.852s  loss=0.3054  iou=0.7728\n",
      " Batch 4: time=3.549s  loss=0.2737  iou=0.8411\n",
      " Batch 5: time=3.616s  loss=0.2806  iou=0.8426\n",
      " Batch 6: time=4.489s  loss=0.2924  iou=0.7929\n",
      " Batch 7: time=4.274s  loss=0.2686  iou=0.8658\n",
      " Batch 8: time=4.918s  loss=0.2959  iou=0.8301\n",
      " Batch 9: time=5.113s  loss=0.2987  iou=0.7972\n",
      " Batch 10: time=4.816s  loss=0.2664  iou=0.8923\n",
      " Batch 11: time=3.623s  loss=0.2773  iou=0.9014\n",
      " Batch 12: time=3.553s  loss=0.3056  iou=0.8000\n",
      " Batch 13: time=3.560s  loss=0.2958  iou=0.8089\n",
      " Batch 14: time=3.594s  loss=0.2803  iou=0.8174\n",
      " Batch 15: time=3.571s  loss=0.2957  iou=0.8447\n",
      " Batch 16: time=3.517s  loss=0.3191  iou=0.7427\n",
      " Batch 17: time=3.534s  loss=0.2973  iou=0.7871\n",
      " Batch 18: time=3.529s  loss=0.2595  iou=0.8762\n",
      " Batch 19: time=3.503s  loss=0.2528  iou=0.9061\n",
      " Batch 20: time=3.563s  loss=0.2784  iou=0.8596\n",
      " Batch 21: time=3.586s  loss=0.3193  iou=0.7572\n",
      " Batch 22: time=3.534s  loss=0.2735  iou=0.8362\n",
      " Batch 23: time=3.605s  loss=0.2910  iou=0.7975\n",
      " Batch 24: time=3.538s  loss=0.2596  iou=0.8798\n",
      " Batch 25: time=3.584s  loss=0.2766  iou=0.8603\n",
      " Batch 26: time=3.595s  loss=0.2467  iou=0.9185\n",
      " Batch 27: time=3.571s  loss=0.2581  iou=0.8927\n",
      " Batch 28: time=3.556s  loss=0.2670  iou=0.8621\n",
      " Batch 29: time=3.711s  loss=0.2588  iou=0.8852\n",
      " Batch 30: time=3.557s  loss=0.2689  iou=0.8271\n",
      " Batch 31: time=3.561s  loss=0.2660  iou=0.8375\n",
      " Batch 32: time=3.526s  loss=0.2765  iou=0.8634\n",
      " Batch 33: time=3.567s  loss=0.2744  iou=0.8183\n",
      " Batch 34: time=3.730s  loss=0.2608  iou=0.8557\n",
      " Batch 35: time=3.739s  loss=0.2559  iou=0.8642\n",
      " Batch 36: time=3.654s  loss=0.2706  iou=0.8680\n",
      " Batch 37: time=3.575s  loss=0.2631  iou=0.8716\n",
      " Batch 38: time=3.795s  loss=0.2418  iou=0.9137\n",
      " Batch 39: time=3.623s  loss=0.2670  iou=0.8699\n",
      " Batch 40: time=3.609s  loss=0.2393  iou=0.8995\n",
      " Batch 41: time=3.589s  loss=0.2798  iou=0.8255\n",
      " Batch 42: time=3.573s  loss=0.2469  iou=0.8917\n",
      " Batch 43: time=3.569s  loss=0.2827  iou=0.8081\n",
      " Batch 44: time=3.555s  loss=0.2695  iou=0.8483\n",
      " Batch 45: time=3.600s  loss=0.2600  iou=0.8763\n",
      " Batch 46: time=3.597s  loss=0.2623  iou=0.8581\n",
      " Batch 47: time=3.639s  loss=0.2716  iou=0.8467\n",
      " Batch 48: time=3.590s  loss=0.2659  iou=0.8563\n",
      " Batch 49: time=3.625s  loss=0.2525  iou=0.8728\n",
      " Batch 50: time=3.703s  loss=0.2586  iou=0.8651\n",
      " Batch 51: time=3.826s  loss=0.2388  iou=0.9029\n",
      " Batch 52: time=3.634s  loss=0.3290  iou=0.7271\n",
      " Batch 53: time=3.633s  loss=0.2550  iou=0.8476\n",
      " Batch 54: time=3.660s  loss=0.2564  iou=0.8491\n",
      " Batch 55: time=3.619s  loss=0.2523  iou=0.8816\n",
      " Batch 56: time=3.591s  loss=0.2565  iou=0.8587\n",
      " Batch 57: time=3.665s  loss=0.2735  iou=0.8388\n",
      " Batch 58: time=3.627s  loss=0.2362  iou=0.8910\n",
      " Batch 59: time=3.647s  loss=0.2488  iou=0.8901\n",
      " Batch 60: time=3.646s  loss=0.2591  iou=0.8517\n",
      " Batch 61: time=3.665s  loss=0.3294  iou=0.7300\n",
      " Batch 62: time=3.649s  loss=0.2399  iou=0.8899\n",
      " Batch 63: time=3.684s  loss=0.4027  iou=0.5594\n",
      " Batch 64: time=3.757s  loss=0.3503  iou=0.6625\n",
      " Batch 65: time=3.619s  loss=0.2362  iou=0.9050\n",
      " Batch 66: time=3.616s  loss=0.2476  iou=0.8466\n",
      " Batch 67: time=3.646s  loss=0.2576  iou=0.8732\n",
      " Batch 68: time=3.674s  loss=0.2689  iou=0.8520\n",
      " Batch 69: time=3.713s  loss=0.2733  iou=0.7921\n",
      " Batch 70: time=3.858s  loss=0.3611  iou=0.6720\n",
      " Batch 71: time=3.974s  loss=0.2536  iou=0.8701\n",
      " Batch 72: time=3.838s  loss=0.2590  iou=0.8381\n",
      " Batch 73: time=3.886s  loss=0.2918  iou=0.7799\n",
      " Batch 74: time=3.934s  loss=0.2537  iou=0.8763\n",
      " Batch 75: time=4.075s  loss=0.2530  iou=0.8361\n",
      " Batch 76: time=3.877s  loss=0.2506  iou=0.8629\n",
      " Batch 77: time=3.907s  loss=0.2441  iou=0.8874\n",
      " Batch 78: time=3.872s  loss=0.2907  iou=0.7737\n",
      " Batch 79: time=3.834s  loss=0.3029  iou=0.7818\n",
      " Batch 80: time=3.813s  loss=0.2640  iou=0.8184\n",
      " Batch 81: time=4.021s  loss=0.2437  iou=0.8752\n",
      " Batch 82: time=4.126s  loss=0.2477  iou=0.8612\n",
      " Batch 83: time=3.860s  loss=0.2114  iou=0.9145\n",
      " Batch 84: time=3.950s  loss=0.2398  iou=0.8669\n",
      " Batch 85: time=4.101s  loss=0.2373  iou=0.8979\n",
      " Batch 86: time=4.090s  loss=0.2479  iou=0.8381\n",
      " Batch 87: time=3.938s  loss=0.2740  iou=0.8065\n",
      " Batch 88: time=3.892s  loss=0.2194  iou=0.9147\n",
      " Batch 89: time=4.075s  loss=0.2196  iou=0.9294\n",
      " Batch 90: time=3.937s  loss=0.2202  iou=0.9139\n",
      " Batch 91: time=3.954s  loss=0.2421  iou=0.8376\n",
      " Batch 92: time=3.920s  loss=0.2344  iou=0.8750\n",
      " Batch 93: time=3.997s  loss=0.2243  iou=0.9005\n",
      " Batch 94: time=3.967s  loss=0.3236  iou=0.6994\n",
      " Batch 95: time=3.988s  loss=0.2788  iou=0.8197\n",
      " Batch 96: time=3.952s  loss=0.2629  iou=0.8171\n",
      " Batch 97: time=4.571s  loss=0.2301  iou=0.8895\n",
      " Batch 98: time=5.014s  loss=0.3117  iou=0.7510\n",
      " Batch 99: time=4.286s  loss=0.2427  iou=0.8858\n",
      " Batch 100: time=3.946s  loss=0.2991  iou=0.7483\n",
      " Batch 101: time=3.794s  loss=0.2379  iou=0.8696\n",
      " Batch 102: time=3.793s  loss=0.2224  iou=0.8994\n",
      " Batch 103: time=3.804s  loss=0.2748  iou=0.7846\n",
      " Batch 104: time=3.776s  loss=0.2275  iou=0.8853\n",
      " Batch 105: time=3.785s  loss=0.2427  iou=0.8438\n",
      " Batch 106: time=3.914s  loss=0.2373  iou=0.8803\n",
      " Batch 107: time=3.749s  loss=0.2401  iou=0.8475\n",
      " Batch 108: time=3.891s  loss=0.2362  iou=0.8503\n",
      " Batch 109: time=3.823s  loss=0.2160  iou=0.8987\n",
      " Batch 110: time=3.800s  loss=0.2208  iou=0.9035\n",
      " Batch 111: time=3.810s  loss=0.2232  iou=0.8950\n",
      " Batch 112: time=3.805s  loss=0.2250  iou=0.8968\n",
      " Batch 113: time=2.021s  loss=0.2585  iou=0.8716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRABHAKAR\\AppData\\Local\\Temp\\ipykernel_10036\\1219489951.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Completed:\n",
      "  Time: 501.14 sec\n",
      "  Train Loss: 0.2661 | Train IoU: 0.8427\n",
      "  Val Loss:   0.2249 | Val IoU:   0.8891\n",
      "\n",
      "================= EPOCH 2/10 =================\n",
      " Batch 1: time=4.567s  loss=0.2250  iou=0.8679\n",
      " Batch 2: time=4.679s  loss=0.2252  iou=0.8628\n",
      " Batch 3: time=4.263s  loss=0.2221  iou=0.8955\n",
      " Batch 4: time=4.537s  loss=0.2296  iou=0.8795\n",
      " Batch 5: time=4.501s  loss=0.2516  iou=0.8242\n",
      " Batch 6: time=4.388s  loss=0.2677  iou=0.7889\n",
      " Batch 7: time=4.408s  loss=0.2239  iou=0.8918\n",
      " Batch 8: time=4.478s  loss=0.2166  iou=0.9116\n",
      " Batch 9: time=4.503s  loss=0.2295  iou=0.8871\n",
      " Batch 10: time=4.570s  loss=0.2233  iou=0.9065\n",
      " Batch 11: time=4.633s  loss=0.2303  iou=0.8664\n",
      " Batch 12: time=4.382s  loss=0.2088  iou=0.9169\n",
      " Batch 13: time=4.443s  loss=0.2188  iou=0.9206\n",
      " Batch 14: time=4.603s  loss=0.2792  iou=0.7947\n",
      " Batch 15: time=4.757s  loss=0.2772  iou=0.8130\n",
      " Batch 16: time=4.980s  loss=0.2419  iou=0.8566\n",
      " Batch 17: time=5.404s  loss=0.2245  iou=0.8978\n",
      " Batch 18: time=4.601s  loss=0.2506  iou=0.8077\n",
      " Batch 19: time=4.601s  loss=0.2008  iou=0.9416\n",
      " Batch 20: time=4.480s  loss=0.2212  iou=0.8886\n",
      " Batch 21: time=4.525s  loss=0.2156  iou=0.8940\n",
      " Batch 22: time=4.757s  loss=0.2203  iou=0.8883\n",
      " Batch 23: time=4.498s  loss=0.2192  iou=0.9026\n",
      " Batch 24: time=4.751s  loss=0.2723  iou=0.7531\n",
      " Batch 25: time=4.288s  loss=0.2163  iou=0.8795\n",
      " Batch 26: time=4.158s  loss=0.2238  iou=0.8947\n",
      " Batch 27: time=4.038s  loss=0.2316  iou=0.8569\n",
      " Batch 28: time=3.975s  loss=0.2094  iou=0.9042\n",
      " Batch 29: time=4.088s  loss=0.2167  iou=0.8898\n",
      " Batch 30: time=3.991s  loss=0.1918  iou=0.9327\n",
      " Batch 31: time=3.975s  loss=0.2171  iou=0.8830\n",
      " Batch 32: time=4.447s  loss=0.2373  iou=0.8452\n",
      " Batch 33: time=4.139s  loss=0.2130  iou=0.9209\n",
      " Batch 34: time=4.220s  loss=0.2028  iou=0.9237\n",
      " Batch 35: time=4.348s  loss=0.2126  iou=0.8876\n",
      " Batch 36: time=4.180s  loss=0.2602  iou=0.8057\n",
      " Batch 37: time=4.121s  loss=0.2226  iou=0.8915\n",
      " Batch 38: time=4.191s  loss=0.2568  iou=0.7893\n",
      " Batch 39: time=4.389s  loss=0.2224  iou=0.8721\n",
      " Batch 40: time=4.564s  loss=0.2065  iou=0.9050\n",
      " Batch 41: time=4.415s  loss=0.2020  iou=0.9182\n",
      " Batch 42: time=4.260s  loss=0.2358  iou=0.8605\n",
      " Batch 43: time=4.265s  loss=0.2118  iou=0.8929\n",
      " Batch 44: time=4.270s  loss=0.2155  iou=0.8957\n",
      " Batch 45: time=4.463s  loss=0.2240  iou=0.8826\n",
      " Batch 46: time=4.221s  loss=0.2178  iou=0.8863\n",
      " Batch 47: time=4.260s  loss=0.2307  iou=0.8506\n",
      " Batch 48: time=4.294s  loss=0.2283  iou=0.8502\n",
      " Batch 49: time=4.225s  loss=0.2034  iou=0.9098\n",
      " Batch 50: time=4.247s  loss=0.2083  iou=0.8795\n",
      " Batch 51: time=4.206s  loss=0.2350  iou=0.8577\n",
      " Batch 52: time=4.424s  loss=0.2212  iou=0.8878\n",
      " Batch 53: time=4.222s  loss=0.2091  iou=0.9101\n",
      " Batch 54: time=4.463s  loss=0.2232  iou=0.8772\n",
      " Batch 55: time=4.213s  loss=0.2087  iou=0.8949\n",
      " Batch 56: time=4.228s  loss=0.2290  iou=0.8574\n",
      " Batch 57: time=4.096s  loss=0.2246  iou=0.8745\n",
      " Batch 58: time=4.096s  loss=0.2414  iou=0.8489\n",
      " Batch 59: time=4.237s  loss=0.2045  iou=0.8801\n",
      " Batch 60: time=4.346s  loss=0.2260  iou=0.8626\n",
      " Batch 61: time=4.209s  loss=0.2143  iou=0.8922\n",
      " Batch 62: time=4.148s  loss=0.2756  iou=0.7446\n",
      " Batch 63: time=4.366s  loss=0.2573  iou=0.8286\n",
      " Batch 64: time=4.475s  loss=0.2123  iou=0.8906\n",
      " Batch 65: time=4.273s  loss=0.2274  iou=0.8628\n",
      " Batch 66: time=4.085s  loss=0.2017  iou=0.9104\n",
      " Batch 67: time=4.420s  loss=0.1926  iou=0.9296\n",
      " Batch 68: time=4.266s  loss=0.2352  iou=0.8478\n",
      " Batch 69: time=4.118s  loss=0.1903  iou=0.9196\n",
      " Batch 70: time=4.124s  loss=0.1955  iou=0.9184\n",
      " Batch 71: time=4.244s  loss=0.2072  iou=0.9048\n",
      " Batch 72: time=4.222s  loss=0.2100  iou=0.9123\n",
      " Batch 73: time=4.101s  loss=0.2038  iou=0.8939\n",
      " Batch 74: time=4.271s  loss=0.2089  iou=0.8861\n",
      " Batch 75: time=4.185s  loss=0.2054  iou=0.8944\n",
      " Batch 76: time=4.184s  loss=0.2291  iou=0.8743\n",
      " Batch 77: time=4.157s  loss=0.1926  iou=0.9245\n",
      " Batch 78: time=4.046s  loss=0.2371  iou=0.7901\n",
      " Batch 79: time=4.227s  loss=0.2149  iou=0.8667\n",
      " Batch 80: time=4.326s  loss=0.2191  iou=0.8761\n",
      " Batch 81: time=4.124s  loss=0.1987  iou=0.9146\n",
      " Batch 82: time=4.013s  loss=0.2234  iou=0.8449\n",
      " Batch 83: time=4.025s  loss=0.2086  iou=0.8647\n",
      " Batch 84: time=4.030s  loss=0.1901  iou=0.9249\n",
      " Batch 85: time=4.198s  loss=0.1941  iou=0.9019\n",
      " Batch 86: time=3.967s  loss=0.2046  iou=0.8945\n",
      " Batch 87: time=3.936s  loss=0.1995  iou=0.9121\n",
      " Batch 88: time=4.001s  loss=0.2218  iou=0.8672\n",
      " Batch 89: time=4.040s  loss=0.1933  iou=0.9016\n",
      " Batch 90: time=4.175s  loss=0.2181  iou=0.8592\n",
      " Batch 91: time=4.079s  loss=0.1890  iou=0.9207\n",
      " Batch 92: time=4.018s  loss=0.2253  iou=0.8477\n",
      " Batch 93: time=4.308s  loss=0.2365  iou=0.8208\n",
      " Batch 94: time=4.199s  loss=0.2220  iou=0.8474\n",
      " Batch 95: time=4.086s  loss=0.2073  iou=0.8828\n",
      " Batch 96: time=4.343s  loss=0.1993  iou=0.8945\n",
      " Batch 97: time=4.055s  loss=0.2417  iou=0.8410\n",
      " Batch 98: time=4.088s  loss=0.1939  iou=0.9108\n",
      " Batch 99: time=3.913s  loss=0.1835  iou=0.9329\n",
      " Batch 100: time=4.204s  loss=0.2621  iou=0.8344\n",
      " Batch 101: time=3.926s  loss=0.1924  iou=0.9228\n",
      " Batch 102: time=3.931s  loss=0.2102  iou=0.8774\n",
      " Batch 103: time=3.994s  loss=0.1911  iou=0.9002\n",
      " Batch 104: time=3.968s  loss=0.2037  iou=0.8861\n",
      " Batch 105: time=4.312s  loss=0.1878  iou=0.9133\n",
      " Batch 106: time=4.032s  loss=0.2009  iou=0.9090\n",
      " Batch 107: time=4.283s  loss=0.1911  iou=0.9014\n",
      " Batch 108: time=4.020s  loss=0.2214  iou=0.8458\n",
      " Batch 109: time=4.168s  loss=0.1863  iou=0.9243\n",
      " Batch 110: time=3.969s  loss=0.2133  iou=0.8607\n",
      " Batch 111: time=4.006s  loss=0.2028  iou=0.8692\n",
      " Batch 112: time=4.136s  loss=0.2075  iou=0.8642\n",
      " Batch 113: time=2.169s  loss=0.2531  iou=0.7577\n",
      "\n",
      "Epoch 2 Completed:\n",
      "  Time: 560.32 sec\n",
      "  Train Loss: 0.2190 | Train IoU: 0.8773\n",
      "  Val Loss:   0.1919 | Val IoU:   0.8933\n",
      "\n",
      "================= EPOCH 3/10 =================\n",
      " Batch 1: time=4.294s  loss=0.2130  iou=0.8564\n",
      " Batch 2: time=4.146s  loss=0.2280  iou=0.8456\n",
      " Batch 3: time=3.990s  loss=0.1871  iou=0.8953\n",
      " Batch 4: time=4.137s  loss=0.2075  iou=0.8664\n",
      " Batch 5: time=4.028s  loss=0.2063  iou=0.8789\n",
      " Batch 6: time=4.340s  loss=0.2643  iou=0.7958\n",
      " Batch 7: time=4.180s  loss=0.2476  iou=0.8479\n",
      " Batch 8: time=4.026s  loss=0.1885  iou=0.9119\n",
      " Batch 9: time=4.081s  loss=0.1846  iou=0.9049\n",
      " Batch 10: time=4.205s  loss=0.2033  iou=0.8695\n",
      " Batch 11: time=4.168s  loss=0.1842  iou=0.9032\n",
      " Batch 12: time=4.242s  loss=0.1876  iou=0.8918\n",
      " Batch 13: time=4.046s  loss=0.2598  iou=0.7755\n",
      " Batch 14: time=4.459s  loss=0.1954  iou=0.8948\n",
      " Batch 15: time=4.169s  loss=0.1914  iou=0.8973\n",
      " Batch 16: time=4.057s  loss=0.2198  iou=0.8648\n",
      " Batch 17: time=4.111s  loss=0.1993  iou=0.8916\n",
      " Batch 18: time=4.113s  loss=0.2527  iou=0.8701\n",
      " Batch 19: time=4.160s  loss=0.1861  iou=0.9409\n",
      " Batch 20: time=4.417s  loss=0.2056  iou=0.8664\n",
      " Batch 21: time=4.171s  loss=0.1807  iou=0.9298\n",
      " Batch 22: time=4.130s  loss=0.1891  iou=0.9103\n",
      " Batch 23: time=4.122s  loss=0.1840  iou=0.9273\n",
      " Batch 24: time=4.180s  loss=0.1935  iou=0.9178\n",
      " Batch 25: time=4.018s  loss=0.1832  iou=0.9160\n",
      " Batch 26: time=4.174s  loss=0.2033  iou=0.8669\n",
      " Batch 27: time=4.314s  loss=0.1800  iou=0.9150\n",
      " Batch 28: time=4.091s  loss=0.1643  iou=0.9464\n",
      " Batch 29: time=4.060s  loss=0.1651  iou=0.9509\n",
      " Batch 30: time=4.245s  loss=0.1822  iou=0.9262\n",
      " Batch 31: time=4.152s  loss=0.1788  iou=0.9162\n",
      " Batch 32: time=4.120s  loss=0.1838  iou=0.9116\n",
      " Batch 33: time=4.079s  loss=0.1994  iou=0.8625\n",
      " Batch 34: time=4.135s  loss=0.1772  iou=0.9131\n",
      " Batch 35: time=4.018s  loss=0.2167  iou=0.8517\n",
      " Batch 36: time=4.077s  loss=0.1870  iou=0.8857\n",
      " Batch 37: time=4.120s  loss=0.2240  iou=0.8253\n",
      " Batch 38: time=3.987s  loss=0.1767  iou=0.9106\n",
      " Batch 39: time=4.214s  loss=0.1853  iou=0.8995\n",
      " Batch 40: time=4.365s  loss=0.1571  iou=0.9572\n",
      " Batch 41: time=4.199s  loss=0.1890  iou=0.9011\n",
      " Batch 42: time=3.985s  loss=0.1863  iou=0.8997\n",
      " Batch 43: time=3.937s  loss=0.2031  iou=0.8656\n",
      " Batch 44: time=4.158s  loss=0.1736  iou=0.9136\n",
      " Batch 45: time=4.027s  loss=0.1959  iou=0.8804\n",
      " Batch 46: time=4.110s  loss=0.1899  iou=0.9194\n",
      " Batch 47: time=4.168s  loss=0.1808  iou=0.9123\n",
      " Batch 48: time=4.130s  loss=0.1735  iou=0.9178\n",
      " Batch 49: time=4.127s  loss=0.1645  iou=0.9290\n",
      " Batch 50: time=4.117s  loss=0.2844  iou=0.7000\n",
      " Batch 51: time=4.157s  loss=0.1601  iou=0.9573\n",
      " Batch 52: time=4.042s  loss=0.1661  iou=0.9410\n",
      " Batch 53: time=4.060s  loss=0.1640  iou=0.9485\n",
      " Batch 54: time=4.089s  loss=0.2000  iou=0.8749\n",
      " Batch 55: time=4.122s  loss=0.1924  iou=0.9175\n",
      " Batch 56: time=4.085s  loss=0.1780  iou=0.9180\n",
      " Batch 57: time=4.068s  loss=0.1818  iou=0.9292\n",
      " Batch 58: time=4.311s  loss=0.1793  iou=0.9057\n",
      " Batch 59: time=4.059s  loss=0.1639  iou=0.9424\n",
      " Batch 60: time=4.171s  loss=0.2073  iou=0.8719\n",
      " Batch 61: time=4.090s  loss=0.1585  iou=0.9417\n",
      " Batch 62: time=4.511s  loss=0.1755  iou=0.9092\n",
      " Batch 63: time=4.183s  loss=0.1718  iou=0.9135\n",
      " Batch 64: time=3.989s  loss=0.1802  iou=0.8893\n",
      " Batch 65: time=4.334s  loss=0.2081  iou=0.8455\n",
      " Batch 66: time=4.083s  loss=0.1686  iou=0.9149\n",
      " Batch 67: time=4.172s  loss=0.1807  iou=0.9032\n",
      " Batch 68: time=4.185s  loss=0.1854  iou=0.9395\n",
      " Batch 69: time=4.068s  loss=0.1809  iou=0.8990\n",
      " Batch 70: time=4.135s  loss=0.1916  iou=0.8765\n",
      " Batch 71: time=4.159s  loss=0.1695  iou=0.9283\n",
      " Batch 72: time=4.012s  loss=0.1723  iou=0.9121\n",
      " Batch 73: time=4.262s  loss=0.2135  iou=0.8759\n",
      " Batch 74: time=4.020s  loss=0.1700  iou=0.9094\n",
      " Batch 75: time=4.092s  loss=0.1599  iou=0.9465\n",
      " Batch 76: time=4.279s  loss=0.1610  iou=0.9363\n",
      " Batch 77: time=4.001s  loss=0.1678  iou=0.9369\n",
      " Batch 78: time=4.081s  loss=0.1707  iou=0.9036\n",
      " Batch 79: time=4.126s  loss=0.1610  iou=0.9333\n",
      " Batch 80: time=4.131s  loss=0.1558  iou=0.9535\n",
      " Batch 81: time=4.131s  loss=0.1672  iou=0.9207\n",
      " Batch 82: time=4.189s  loss=0.1576  iou=0.9355\n",
      " Batch 83: time=4.027s  loss=0.1846  iou=0.8887\n",
      " Batch 84: time=4.040s  loss=0.2131  iou=0.8548\n",
      " Batch 85: time=4.185s  loss=0.1840  iou=0.9053\n",
      " Batch 86: time=4.312s  loss=0.1660  iou=0.9277\n",
      " Batch 87: time=4.131s  loss=0.1609  iou=0.9409\n",
      " Batch 88: time=3.930s  loss=0.1830  iou=0.9457\n",
      " Batch 89: time=4.140s  loss=0.1649  iou=0.9301\n",
      " Batch 90: time=4.125s  loss=0.1757  iou=0.9107\n",
      " Batch 91: time=4.125s  loss=0.2280  iou=0.8206\n",
      " Batch 92: time=4.139s  loss=0.1666  iou=0.9300\n",
      " Batch 93: time=4.004s  loss=0.1631  iou=0.9321\n",
      " Batch 94: time=4.254s  loss=0.1602  iou=0.9320\n",
      " Batch 95: time=4.017s  loss=0.1635  iou=0.9219\n",
      " Batch 96: time=4.026s  loss=0.1599  iou=0.9186\n",
      " Batch 97: time=4.037s  loss=0.1864  iou=0.8898\n",
      " Batch 98: time=4.061s  loss=0.1718  iou=0.9029\n",
      " Batch 99: time=3.976s  loss=0.1609  iou=0.9331\n",
      " Batch 100: time=4.135s  loss=0.2362  iou=0.8066\n",
      " Batch 101: time=3.954s  loss=0.1533  iou=0.9366\n",
      " Batch 102: time=4.285s  loss=0.1656  iou=0.9274\n",
      " Batch 103: time=4.031s  loss=0.1732  iou=0.9084\n",
      " Batch 104: time=3.969s  loss=0.1656  iou=0.9144\n",
      " Batch 105: time=4.324s  loss=0.1651  iou=0.9134\n",
      " Batch 106: time=4.003s  loss=0.2200  iou=0.8182\n",
      " Batch 107: time=4.188s  loss=0.1527  iou=0.9315\n",
      " Batch 108: time=4.077s  loss=0.1705  iou=0.9112\n",
      " Batch 109: time=3.950s  loss=0.2067  iou=0.8812\n",
      " Batch 110: time=4.314s  loss=0.1913  iou=0.8830\n",
      " Batch 111: time=4.100s  loss=0.2171  iou=0.8365\n",
      " Batch 112: time=4.239s  loss=0.1764  iou=0.8874\n",
      " Batch 113: time=2.127s  loss=0.2244  iou=0.7788\n",
      "\n",
      "Epoch 3 Completed:\n",
      "  Time: 543.59 sec\n",
      "  Train Loss: 0.1867 | Train IoU: 0.8991\n",
      "  Val Loss:   0.1638 | Val IoU:   0.9091\n",
      "\n",
      "================= EPOCH 4/10 =================\n",
      " Batch 1: time=4.033s  loss=0.1631  iou=0.9222\n",
      " Batch 2: time=4.069s  loss=0.1646  iou=0.9094\n",
      " Batch 3: time=4.081s  loss=0.1832  iou=0.8694\n",
      " Batch 4: time=4.285s  loss=0.1938  iou=0.8748\n",
      " Batch 5: time=4.009s  loss=0.1791  iou=0.8793\n",
      " Batch 6: time=4.223s  loss=0.1800  iou=0.8885\n",
      " Batch 7: time=4.346s  loss=0.2059  iou=0.8395\n",
      " Batch 8: time=4.304s  loss=0.1847  iou=0.8699\n",
      " Batch 9: time=4.143s  loss=0.1758  iou=0.8916\n",
      " Batch 10: time=4.161s  loss=0.1887  iou=0.8987\n",
      " Batch 11: time=4.440s  loss=0.1637  iou=0.9134\n",
      " Batch 12: time=3.987s  loss=0.1797  iou=0.8763\n",
      " Batch 13: time=4.196s  loss=0.1805  iou=0.8675\n",
      " Batch 14: time=4.130s  loss=0.1971  iou=0.8349\n",
      " Batch 15: time=4.083s  loss=0.1842  iou=0.8749\n",
      " Batch 16: time=4.134s  loss=0.1899  iou=0.8909\n",
      " Batch 17: time=4.448s  loss=0.1597  iou=0.9195\n",
      " Batch 18: time=4.062s  loss=0.1383  iou=0.9480\n",
      " Batch 19: time=4.297s  loss=0.1773  iou=0.8748\n",
      " Batch 20: time=4.097s  loss=0.1450  iou=0.9375\n",
      " Batch 21: time=4.015s  loss=0.1719  iou=0.8873\n",
      " Batch 22: time=4.022s  loss=0.1614  iou=0.9056\n",
      " Batch 23: time=4.096s  loss=0.1804  iou=0.8956\n",
      " Batch 24: time=4.380s  loss=0.3090  iou=0.5775\n",
      " Batch 25: time=4.063s  loss=0.1558  iou=0.9253\n",
      " Batch 26: time=4.300s  loss=0.1746  iou=0.9097\n",
      " Batch 27: time=4.136s  loss=0.1830  iou=0.8747\n",
      " Batch 28: time=4.232s  loss=0.1703  iou=0.8972\n",
      " Batch 29: time=4.059s  loss=0.1452  iou=0.9366\n",
      " Batch 30: time=4.171s  loss=0.1769  iou=0.8873\n",
      " Batch 31: time=4.193s  loss=0.1753  iou=0.9185\n",
      " Batch 32: time=3.976s  loss=0.1717  iou=0.8853\n",
      " Batch 33: time=4.203s  loss=0.1817  iou=0.8676\n",
      " Batch 34: time=4.106s  loss=0.1717  iou=0.9110\n",
      " Batch 35: time=4.144s  loss=0.1772  iou=0.8768\n",
      " Batch 36: time=3.985s  loss=0.1884  iou=0.8709\n",
      " Batch 37: time=3.933s  loss=0.1809  iou=0.9035\n",
      " Batch 38: time=4.355s  loss=0.1575  iou=0.9237\n",
      " Batch 39: time=4.296s  loss=0.1736  iou=0.8697\n",
      " Batch 40: time=3.908s  loss=0.1762  iou=0.8677\n",
      " Batch 41: time=3.977s  loss=0.2131  iou=0.8148\n",
      " Batch 42: time=4.005s  loss=0.1594  iou=0.9224\n",
      " Batch 43: time=4.043s  loss=0.1491  iou=0.9227\n",
      " Batch 44: time=4.019s  loss=0.1698  iou=0.9003\n",
      " Batch 45: time=4.061s  loss=0.1584  iou=0.9031\n",
      " Batch 46: time=4.460s  loss=0.1447  iou=0.9425\n",
      " Batch 47: time=4.223s  loss=0.1570  iou=0.9130\n",
      " Batch 48: time=4.235s  loss=0.1592  iou=0.9046\n",
      " Batch 49: time=4.111s  loss=0.1772  iou=0.8810\n",
      " Batch 50: time=4.158s  loss=0.1531  iou=0.9220\n",
      " Batch 51: time=4.014s  loss=0.1786  iou=0.8703\n",
      " Batch 52: time=4.102s  loss=0.1812  iou=0.9131\n",
      " Batch 53: time=4.078s  loss=0.1960  iou=0.8604\n",
      " Batch 54: time=4.158s  loss=0.1867  iou=0.8644\n",
      " Batch 55: time=4.147s  loss=0.1521  iou=0.9292\n",
      " Batch 56: time=4.152s  loss=0.1400  iou=0.9379\n",
      " Batch 57: time=3.990s  loss=0.1487  iou=0.9219\n",
      " Batch 58: time=4.090s  loss=0.1589  iou=0.8976\n",
      " Batch 59: time=4.011s  loss=0.1827  iou=0.8739\n",
      " Batch 60: time=4.109s  loss=0.1507  iou=0.9206\n",
      " Batch 61: time=4.332s  loss=0.1550  iou=0.9364\n",
      " Batch 62: time=4.130s  loss=0.1601  iou=0.9067\n",
      " Batch 63: time=4.175s  loss=0.1822  iou=0.9112\n",
      " Batch 64: time=4.070s  loss=0.1530  iou=0.9248\n",
      " Batch 65: time=4.078s  loss=0.1311  iou=0.9533\n",
      " Batch 66: time=4.149s  loss=0.1346  iou=0.9421\n",
      " Batch 67: time=4.115s  loss=0.1695  iou=0.8981\n",
      " Batch 68: time=4.111s  loss=0.1303  iou=0.9484\n",
      " Batch 69: time=4.045s  loss=0.1570  iou=0.9092\n",
      " Batch 70: time=4.387s  loss=0.1681  iou=0.8882\n",
      " Batch 71: time=4.155s  loss=0.1629  iou=0.9005\n",
      " Batch 72: time=4.198s  loss=0.1673  iou=0.9212\n",
      " Batch 73: time=4.100s  loss=0.1596  iou=0.9123\n",
      " Batch 74: time=4.113s  loss=0.1473  iou=0.9089\n",
      " Batch 75: time=4.068s  loss=0.1823  iou=0.8856\n",
      " Batch 76: time=4.177s  loss=0.1385  iou=0.9394\n",
      " Batch 77: time=4.089s  loss=0.1287  iou=0.9550\n",
      " Batch 78: time=4.155s  loss=0.1403  iou=0.9461\n",
      " Batch 79: time=4.082s  loss=0.1509  iou=0.9105\n",
      " Batch 80: time=4.060s  loss=0.1926  iou=0.8531\n",
      " Batch 81: time=4.062s  loss=0.1389  iou=0.9390\n",
      " Batch 82: time=4.356s  loss=0.2086  iou=0.8521\n",
      " Batch 83: time=4.147s  loss=0.1540  iou=0.9076\n",
      " Batch 84: time=4.299s  loss=0.1394  iou=0.9410\n",
      " Batch 85: time=4.012s  loss=0.1566  iou=0.8960\n",
      " Batch 86: time=4.144s  loss=0.1554  iou=0.9349\n",
      " Batch 87: time=3.995s  loss=0.1557  iou=0.9064\n",
      " Batch 88: time=4.089s  loss=0.1701  iou=0.8889\n",
      " Batch 89: time=4.246s  loss=0.1441  iou=0.9360\n",
      " Batch 90: time=4.148s  loss=0.1672  iou=0.8774\n",
      " Batch 91: time=4.092s  loss=0.1862  iou=0.8457\n",
      " Batch 92: time=4.098s  loss=0.1576  iou=0.9094\n",
      " Batch 93: time=4.110s  loss=0.1617  iou=0.8885\n",
      " Batch 94: time=3.944s  loss=0.1562  iou=0.8945\n",
      " Batch 95: time=4.028s  loss=0.1534  iou=0.9277\n",
      " Batch 96: time=4.230s  loss=0.1574  iou=0.8905\n",
      " Batch 97: time=3.999s  loss=0.1288  iou=0.9497\n",
      " Batch 98: time=3.996s  loss=0.1602  iou=0.9008\n",
      " Batch 99: time=4.193s  loss=0.1693  iou=0.8927\n",
      " Batch 100: time=4.048s  loss=0.1485  iou=0.9366\n",
      " Batch 101: time=4.179s  loss=0.1444  iou=0.9203\n",
      " Batch 102: time=4.025s  loss=0.1647  iou=0.9154\n",
      " Batch 103: time=4.031s  loss=0.1515  iou=0.9235\n",
      " Batch 104: time=4.194s  loss=0.1484  iou=0.9163\n",
      " Batch 105: time=4.046s  loss=0.1566  iou=0.9145\n",
      " Batch 106: time=4.108s  loss=0.1392  iou=0.9314\n",
      " Batch 107: time=4.285s  loss=0.1575  iou=0.9276\n",
      " Batch 108: time=4.010s  loss=0.1346  iou=0.9438\n",
      " Batch 109: time=4.158s  loss=0.1399  iou=0.9291\n",
      " Batch 110: time=4.290s  loss=0.1613  iou=0.8758\n",
      " Batch 111: time=4.058s  loss=0.1253  iou=0.9546\n",
      " Batch 112: time=4.026s  loss=0.1397  iou=0.9214\n",
      " Batch 113: time=2.100s  loss=0.1730  iou=0.8873\n",
      "\n",
      "Epoch 4 Completed:\n",
      "  Time: 542.43 sec\n",
      "  Train Loss: 0.1651 | Train IoU: 0.9010\n",
      "  Val Loss:   0.1374 | Val IoU:   0.9265\n",
      "\n",
      "================= EPOCH 5/10 =================\n",
      " Batch 1: time=4.318s  loss=0.1345  iou=0.9389\n",
      " Batch 2: time=4.145s  loss=0.1455  iou=0.9149\n",
      " Batch 3: time=4.448s  loss=0.1492  iou=0.9157\n",
      " Batch 4: time=4.171s  loss=0.1907  iou=0.8711\n",
      " Batch 5: time=4.263s  loss=0.1238  iou=0.9585\n",
      " Batch 6: time=4.094s  loss=0.1248  iou=0.9561\n",
      " Batch 7: time=4.047s  loss=0.1719  iou=0.8813\n",
      " Batch 8: time=3.946s  loss=0.1433  iou=0.9350\n",
      " Batch 9: time=4.304s  loss=0.1744  iou=0.8922\n",
      " Batch 10: time=3.963s  loss=0.1336  iou=0.9295\n",
      " Batch 11: time=4.250s  loss=0.1317  iou=0.9451\n",
      " Batch 12: time=3.995s  loss=0.1283  iou=0.9453\n",
      " Batch 13: time=4.040s  loss=0.1371  iou=0.9119\n",
      " Batch 14: time=4.045s  loss=0.1425  iou=0.9219\n",
      " Batch 15: time=4.066s  loss=0.1202  iou=0.9494\n",
      " Batch 16: time=4.160s  loss=0.1563  iou=0.9060\n",
      " Batch 17: time=4.100s  loss=0.1434  iou=0.9274\n",
      " Batch 18: time=4.099s  loss=0.1628  iou=0.9199\n",
      " Batch 19: time=4.028s  loss=0.1737  iou=0.8942\n",
      " Batch 20: time=4.295s  loss=0.1525  iou=0.9086\n",
      " Batch 21: time=4.044s  loss=0.1359  iou=0.9218\n",
      " Batch 22: time=4.211s  loss=0.1431  iou=0.9274\n",
      " Batch 23: time=4.070s  loss=0.1448  iou=0.9119\n",
      " Batch 24: time=4.159s  loss=0.1372  iou=0.9271\n",
      " Batch 25: time=4.085s  loss=0.1583  iou=0.8845\n",
      " Batch 26: time=4.121s  loss=0.1325  iou=0.9334\n",
      " Batch 27: time=3.987s  loss=0.1390  iou=0.9249\n",
      " Batch 28: time=4.216s  loss=0.1933  iou=0.8447\n",
      " Batch 29: time=4.083s  loss=0.1439  iou=0.9114\n",
      " Batch 30: time=4.214s  loss=0.1351  iou=0.9326\n",
      " Batch 31: time=4.071s  loss=0.1171  iou=0.9673\n",
      " Batch 32: time=3.986s  loss=0.1321  iou=0.9365\n",
      " Batch 33: time=4.405s  loss=0.1933  iou=0.8589\n",
      " Batch 34: time=4.091s  loss=0.1329  iou=0.9460\n",
      " Batch 35: time=4.217s  loss=0.1385  iou=0.9162\n",
      " Batch 36: time=4.108s  loss=0.1424  iou=0.9273\n",
      " Batch 37: time=4.031s  loss=0.1258  iou=0.9511\n",
      " Batch 38: time=4.200s  loss=0.1453  iou=0.9283\n",
      " Batch 39: time=4.263s  loss=0.1623  iou=0.8879\n",
      " Batch 40: time=3.985s  loss=0.1321  iou=0.9269\n",
      " Batch 41: time=4.087s  loss=0.1261  iou=0.9518\n",
      " Batch 42: time=3.959s  loss=0.1432  iou=0.8943\n",
      " Batch 43: time=4.182s  loss=0.1722  iou=0.8810\n",
      " Batch 44: time=4.142s  loss=0.1256  iou=0.9448\n",
      " Batch 45: time=4.455s  loss=0.1241  iou=0.9366\n",
      " Batch 46: time=4.004s  loss=0.1259  iou=0.9384\n",
      " Batch 47: time=4.272s  loss=0.1419  iou=0.9119\n",
      " Batch 48: time=4.005s  loss=0.1384  iou=0.9177\n",
      " Batch 49: time=4.188s  loss=0.1825  iou=0.8561\n",
      " Batch 50: time=4.057s  loss=0.1142  iou=0.9644\n",
      " Batch 51: time=4.251s  loss=0.1374  iou=0.9234\n",
      " Batch 52: time=4.012s  loss=0.1667  iou=0.9376\n",
      " Batch 53: time=4.240s  loss=0.1268  iou=0.9425\n",
      " Batch 54: time=4.333s  loss=0.1333  iou=0.9438\n",
      " Batch 55: time=4.246s  loss=0.1253  iou=0.9387\n",
      " Batch 56: time=4.073s  loss=0.1136  iou=0.9558\n",
      " Batch 57: time=4.297s  loss=0.1327  iou=0.9288\n",
      " Batch 58: time=4.156s  loss=0.1115  iou=0.9631\n",
      " Batch 59: time=4.117s  loss=0.1321  iou=0.9281\n",
      " Batch 60: time=4.164s  loss=0.1332  iou=0.9279\n",
      " Batch 61: time=4.165s  loss=0.1218  iou=0.9519\n",
      " Batch 62: time=4.191s  loss=0.1223  iou=0.9556\n",
      " Batch 63: time=4.001s  loss=0.1147  iou=0.9654\n",
      " Batch 64: time=4.417s  loss=0.1325  iou=0.9206\n",
      " Batch 65: time=4.092s  loss=0.1062  iou=0.9669\n",
      " Batch 66: time=4.250s  loss=0.1422  iou=0.9261\n",
      " Batch 67: time=4.132s  loss=0.1360  iou=0.9064\n",
      " Batch 68: time=4.139s  loss=0.1429  iou=0.9141\n",
      " Batch 69: time=4.143s  loss=0.1540  iou=0.9045\n",
      " Batch 70: time=4.167s  loss=0.1193  iou=0.9467\n",
      " Batch 71: time=4.339s  loss=0.1391  iou=0.9080\n",
      " Batch 72: time=4.146s  loss=0.1203  iou=0.9483\n",
      " Batch 73: time=4.124s  loss=0.1615  iou=0.9341\n",
      " Batch 74: time=4.255s  loss=0.1370  iou=0.9223\n",
      " Batch 75: time=4.408s  loss=0.1438  iou=0.9241\n",
      " Batch 76: time=4.168s  loss=0.1318  iou=0.9203\n",
      " Batch 77: time=4.123s  loss=0.1341  iou=0.9488\n",
      " Batch 78: time=4.047s  loss=0.1284  iou=0.9285\n",
      " Batch 79: time=4.073s  loss=0.1197  iou=0.9531\n",
      " Batch 80: time=4.042s  loss=0.1309  iou=0.9418\n",
      " Batch 81: time=4.191s  loss=0.1112  iou=0.9553\n",
      " Batch 82: time=4.084s  loss=0.1183  iou=0.9464\n",
      " Batch 83: time=4.067s  loss=0.1726  iou=0.8595\n",
      " Batch 84: time=3.968s  loss=0.1367  iou=0.9058\n",
      " Batch 85: time=4.083s  loss=0.1082  iou=0.9654\n",
      " Batch 86: time=4.121s  loss=0.1196  iou=0.9453\n",
      " Batch 87: time=4.047s  loss=0.1137  iou=0.9528\n",
      " Batch 88: time=4.000s  loss=0.1626  iou=0.8926\n",
      " Batch 89: time=4.028s  loss=0.1379  iou=0.9082\n",
      " Batch 90: time=4.101s  loss=0.1268  iou=0.9387\n",
      " Batch 91: time=4.018s  loss=0.1286  iou=0.9316\n",
      " Batch 92: time=4.023s  loss=0.1355  iou=0.9175\n",
      " Batch 93: time=4.041s  loss=0.1901  iou=0.8701\n",
      " Batch 94: time=4.240s  loss=0.1305  iou=0.9252\n",
      " Batch 95: time=4.336s  loss=0.1585  iou=0.8791\n",
      " Batch 96: time=5.097s  loss=0.1614  iou=0.9109\n",
      " Batch 97: time=4.641s  loss=0.1463  iou=0.9256\n",
      " Batch 98: time=4.702s  loss=0.1304  iou=0.9220\n",
      " Batch 99: time=4.574s  loss=0.1196  iou=0.9416\n",
      " Batch 100: time=4.154s  loss=0.1344  iou=0.9198\n",
      " Batch 101: time=4.277s  loss=0.1183  iou=0.9419\n",
      " Batch 102: time=8.567s  loss=0.1561  iou=0.9074\n",
      " Batch 103: time=5.922s  loss=0.1454  iou=0.9045\n",
      " Batch 104: time=7.559s  loss=0.1207  iou=0.9286\n",
      " Batch 105: time=9.826s  loss=0.1459  iou=0.9066\n",
      " Batch 106: time=10.284s  loss=0.1452  iou=0.9102\n",
      " Batch 107: time=10.833s  loss=0.1292  iou=0.9166\n",
      " Batch 108: time=10.792s  loss=0.2227  iou=0.7816\n",
      " Batch 109: time=10.769s  loss=0.1254  iou=0.9279\n",
      " Batch 110: time=10.177s  loss=0.1196  iou=0.9310\n",
      " Batch 111: time=9.736s  loss=0.1256  iou=0.9303\n",
      " Batch 112: time=8.082s  loss=0.1669  iou=0.8504\n",
      " Batch 113: time=4.817s  loss=0.1526  iou=0.8869\n",
      "\n",
      "Epoch 5 Completed:\n",
      "  Time: 644.74 sec\n",
      "  Train Loss: 0.1397 | Train IoU: 0.9221\n",
      "  Val Loss:   0.1247 | Val IoU:   0.9323\n",
      "\n",
      "================= EPOCH 6/10 =================\n",
      " Batch 1: time=9.989s  loss=0.1230  iou=0.9350\n",
      " Batch 2: time=10.032s  loss=0.1643  iou=0.8514\n",
      " Batch 3: time=10.159s  loss=0.1741  iou=0.8884\n",
      " Batch 4: time=9.359s  loss=0.1605  iou=0.8707\n",
      " Batch 5: time=3.831s  loss=0.1325  iou=0.9067\n",
      " Batch 6: time=3.544s  loss=0.1550  iou=0.8829\n",
      " Batch 7: time=3.341s  loss=0.1164  iou=0.9304\n",
      " Batch 8: time=4.218s  loss=0.1234  iou=0.9393\n",
      " Batch 9: time=3.795s  loss=0.1280  iou=0.9228\n",
      " Batch 10: time=4.215s  loss=0.1531  iou=0.8856\n",
      " Batch 11: time=3.877s  loss=0.1309  iou=0.9195\n",
      " Batch 12: time=3.634s  loss=0.1189  iou=0.9351\n",
      " Batch 13: time=3.401s  loss=0.1020  iou=0.9607\n",
      " Batch 14: time=3.440s  loss=0.1218  iou=0.9480\n",
      " Batch 15: time=3.954s  loss=0.1044  iou=0.9606\n",
      " Batch 16: time=5.072s  loss=0.1269  iou=0.9244\n",
      " Batch 17: time=4.359s  loss=0.1038  iou=0.9567\n",
      " Batch 18: time=4.176s  loss=0.1354  iou=0.8993\n",
      " Batch 19: time=6.215s  loss=0.1243  iou=0.9311\n",
      " Batch 20: time=11.791s  loss=0.1212  iou=0.9268\n",
      " Batch 21: time=12.582s  loss=0.1050  iou=0.9582\n",
      " Batch 22: time=12.280s  loss=0.1494  iou=0.9095\n",
      " Batch 23: time=12.048s  loss=0.1287  iou=0.9074\n",
      " Batch 24: time=12.089s  loss=0.1075  iou=0.9524\n",
      " Batch 25: time=12.662s  loss=0.1316  iou=0.9193\n",
      " Batch 26: time=12.441s  loss=0.1381  iou=0.9310\n",
      " Batch 27: time=12.205s  loss=0.1493  iou=0.8816\n",
      " Batch 28: time=11.972s  loss=0.1111  iou=0.9593\n",
      " Batch 29: time=12.402s  loss=0.1225  iou=0.9433\n",
      " Batch 30: time=11.865s  loss=0.1205  iou=0.9202\n",
      " Batch 31: time=12.877s  loss=0.1209  iou=0.9298\n",
      " Batch 32: time=13.712s  loss=0.1111  iou=0.9415\n",
      " Batch 33: time=12.291s  loss=0.1236  iou=0.9311\n",
      " Batch 34: time=12.219s  loss=0.1195  iou=0.9595\n",
      " Batch 35: time=12.226s  loss=0.1342  iou=0.9432\n",
      " Batch 36: time=12.302s  loss=0.1057  iou=0.9544\n",
      " Batch 37: time=11.601s  loss=0.1290  iou=0.9128\n",
      " Batch 38: time=12.387s  loss=0.1092  iou=0.9615\n",
      " Batch 39: time=12.529s  loss=0.1163  iou=0.9538\n",
      " Batch 40: time=12.757s  loss=0.1025  iou=0.9596\n",
      " Batch 41: time=12.137s  loss=0.1297  iou=0.9186\n",
      " Batch 42: time=12.211s  loss=0.1154  iou=0.9397\n",
      " Batch 43: time=12.372s  loss=0.1298  iou=0.9202\n",
      " Batch 44: time=12.112s  loss=0.1444  iou=0.9078\n",
      " Batch 45: time=12.977s  loss=0.2168  iou=0.8268\n",
      " Batch 46: time=12.620s  loss=0.1100  iou=0.9505\n",
      " Batch 47: time=12.957s  loss=0.1237  iou=0.9098\n",
      " Batch 48: time=12.928s  loss=0.1282  iou=0.9105\n",
      " Batch 49: time=12.627s  loss=0.1130  iou=0.9355\n",
      " Batch 50: time=12.907s  loss=0.1169  iou=0.9301\n",
      " Batch 51: time=13.194s  loss=0.1170  iou=0.9282\n",
      " Batch 52: time=12.326s  loss=0.1249  iou=0.9150\n",
      " Batch 53: time=12.361s  loss=0.1408  iou=0.9007\n",
      " Batch 54: time=12.519s  loss=0.1307  iou=0.9284\n",
      " Batch 55: time=12.932s  loss=0.1112  iou=0.9370\n",
      " Batch 56: time=12.919s  loss=0.1155  iou=0.9488\n",
      " Batch 57: time=12.806s  loss=0.1370  iou=0.8946\n",
      " Batch 58: time=13.005s  loss=0.1290  iou=0.9146\n",
      " Batch 59: time=12.378s  loss=0.1157  iou=0.9308\n",
      " Batch 60: time=12.632s  loss=0.1115  iou=0.9393\n",
      " Batch 61: time=12.444s  loss=0.1111  iou=0.9401\n",
      " Batch 62: time=12.172s  loss=0.1110  iou=0.9378\n",
      " Batch 63: time=11.787s  loss=0.1340  iou=0.9055\n",
      " Batch 64: time=11.514s  loss=0.1463  iou=0.8896\n",
      " Batch 65: time=12.998s  loss=0.1020  iou=0.9734\n",
      " Batch 66: time=13.265s  loss=0.1104  iou=0.9417\n",
      " Batch 67: time=13.078s  loss=0.1293  iou=0.9311\n",
      " Batch 68: time=12.614s  loss=0.1255  iou=0.9193\n",
      " Batch 69: time=12.324s  loss=0.1177  iou=0.9216\n",
      " Batch 70: time=12.395s  loss=0.1593  iou=0.8628\n",
      " Batch 71: time=12.338s  loss=0.1218  iou=0.9384\n",
      " Batch 72: time=12.297s  loss=0.1229  iou=0.9171\n",
      " Batch 73: time=12.528s  loss=0.1313  iou=0.9050\n",
      " Batch 74: time=12.642s  loss=0.1565  iou=0.8968\n",
      " Batch 75: time=11.967s  loss=0.1556  iou=0.8764\n",
      " Batch 76: time=12.127s  loss=0.1296  iou=0.9081\n",
      " Batch 77: time=12.755s  loss=0.1250  iou=0.9143\n",
      " Batch 78: time=12.286s  loss=0.1442  iou=0.9030\n",
      " Batch 79: time=12.530s  loss=0.1686  iou=0.8322\n",
      " Batch 80: time=12.386s  loss=0.1258  iou=0.9122\n",
      " Batch 81: time=12.693s  loss=0.1165  iou=0.9222\n",
      " Batch 82: time=12.962s  loss=0.1140  iou=0.9571\n",
      " Batch 83: time=12.594s  loss=0.1296  iou=0.9335\n",
      " Batch 84: time=13.053s  loss=0.1138  iou=0.9295\n",
      " Batch 85: time=12.019s  loss=0.1313  iou=0.9185\n",
      " Batch 86: time=12.278s  loss=0.1391  iou=0.9115\n",
      " Batch 87: time=12.442s  loss=0.1190  iou=0.9320\n",
      " Batch 88: time=12.542s  loss=0.1239  iou=0.9019\n",
      " Batch 89: time=12.456s  loss=0.1147  iou=0.9247\n",
      " Batch 90: time=12.657s  loss=0.1685  iou=0.8794\n",
      " Batch 91: time=12.282s  loss=0.0991  iou=0.9570\n",
      " Batch 92: time=13.198s  loss=0.1043  iou=0.9487\n",
      " Batch 93: time=13.167s  loss=0.1271  iou=0.9125\n",
      " Batch 94: time=12.878s  loss=0.1026  iou=0.9474\n",
      " Batch 95: time=12.848s  loss=0.0997  iou=0.9532\n",
      " Batch 96: time=12.329s  loss=0.1287  iou=0.9052\n",
      " Batch 97: time=11.980s  loss=0.1389  iou=0.8937\n",
      " Batch 98: time=13.590s  loss=0.1090  iou=0.9354\n",
      " Batch 99: time=12.288s  loss=0.1010  iou=0.9465\n",
      " Batch 100: time=8.077s  loss=0.1231  iou=0.9009\n",
      " Batch 101: time=3.927s  loss=0.1083  iou=0.9376\n",
      " Batch 102: time=7.212s  loss=0.0965  iou=0.9536\n",
      " Batch 103: time=8.151s  loss=0.1149  iou=0.9222\n",
      " Batch 104: time=4.056s  loss=0.1019  iou=0.9472\n",
      " Batch 105: time=4.045s  loss=0.1077  iou=0.9376\n",
      " Batch 106: time=4.043s  loss=0.1076  iou=0.9338\n",
      " Batch 107: time=4.395s  loss=0.1741  iou=0.8559\n",
      " Batch 108: time=4.146s  loss=0.0936  iou=0.9630\n",
      " Batch 109: time=9.296s  loss=0.1089  iou=0.9647\n",
      " Batch 110: time=10.491s  loss=0.1305  iou=0.9105\n",
      " Batch 111: time=10.322s  loss=0.1482  iou=0.9247\n",
      " Batch 112: time=10.290s  loss=0.1063  iou=0.9440\n",
      " Batch 113: time=5.117s  loss=0.1276  iou=0.9462\n",
      "\n",
      "Epoch 6 Completed:\n",
      "  Time: 1338.32 sec\n",
      "  Train Loss: 0.1254 | Train IoU: 0.9236\n",
      "  Val Loss:   0.1114 | Val IoU:   0.9477\n",
      "\n",
      "================= EPOCH 7/10 =================\n",
      " Batch 1: time=10.109s  loss=0.1015  iou=0.9671\n",
      " Batch 2: time=10.294s  loss=0.1171  iou=0.9310\n",
      " Batch 3: time=10.260s  loss=0.0952  iou=0.9522\n",
      " Batch 4: time=10.055s  loss=0.1192  iou=0.9316\n",
      " Batch 5: time=10.785s  loss=0.1008  iou=0.9431\n",
      " Batch 6: time=10.233s  loss=0.1284  iou=0.9384\n",
      " Batch 7: time=10.041s  loss=0.1135  iou=0.9215\n",
      " Batch 8: time=10.095s  loss=0.1030  iou=0.9496\n",
      " Batch 9: time=10.362s  loss=0.1465  iou=0.8781\n",
      " Batch 10: time=10.134s  loss=0.1264  iou=0.9423\n",
      " Batch 11: time=10.199s  loss=0.1288  iou=0.9218\n",
      " Batch 12: time=10.510s  loss=0.1056  iou=0.9371\n",
      " Batch 13: time=10.997s  loss=0.0899  iou=0.9685\n",
      " Batch 14: time=10.094s  loss=0.0893  iou=0.9678\n",
      " Batch 15: time=9.954s  loss=0.1288  iou=0.9068\n",
      " Batch 16: time=9.985s  loss=0.1033  iou=0.9374\n",
      " Batch 17: time=9.804s  loss=0.1042  iou=0.9415\n",
      " Batch 18: time=10.254s  loss=0.1286  iou=0.9023\n",
      " Batch 19: time=10.739s  loss=0.1082  iou=0.9653\n",
      " Batch 20: time=11.173s  loss=0.1119  iou=0.9314\n",
      " Batch 21: time=11.121s  loss=0.0952  iou=0.9582\n",
      " Batch 22: time=11.200s  loss=0.1214  iou=0.9298\n",
      " Batch 23: time=11.140s  loss=0.1357  iou=0.9216\n",
      " Batch 24: time=11.223s  loss=0.1199  iou=0.9449\n",
      " Batch 25: time=11.107s  loss=0.1163  iou=0.9252\n",
      " Batch 26: time=11.140s  loss=0.1006  iou=0.9456\n",
      " Batch 27: time=11.067s  loss=0.1050  iou=0.9311\n",
      " Batch 28: time=11.099s  loss=0.0905  iou=0.9593\n",
      " Batch 29: time=10.995s  loss=0.0998  iou=0.9527\n",
      " Batch 30: time=11.038s  loss=0.1015  iou=0.9545\n",
      " Batch 31: time=10.195s  loss=0.0901  iou=0.9655\n",
      " Batch 32: time=10.200s  loss=0.0859  iou=0.9702\n",
      " Batch 33: time=10.286s  loss=0.0933  iou=0.9632\n",
      " Batch 34: time=10.359s  loss=0.1435  iou=0.8874\n",
      " Batch 35: time=10.150s  loss=0.1184  iou=0.9351\n",
      " Batch 36: time=9.770s  loss=0.1058  iou=0.9300\n",
      " Batch 37: time=10.060s  loss=0.1027  iou=0.9590\n",
      " Batch 38: time=10.601s  loss=0.1156  iou=0.9294\n",
      " Batch 39: time=9.877s  loss=0.1322  iou=0.9136\n",
      " Batch 40: time=10.192s  loss=0.0965  iou=0.9526\n",
      " Batch 41: time=10.204s  loss=0.1110  iou=0.9377\n",
      " Batch 42: time=10.154s  loss=0.1059  iou=0.9306\n",
      " Batch 43: time=10.155s  loss=0.0978  iou=0.9497\n",
      " Batch 44: time=10.100s  loss=0.1053  iou=0.9324\n",
      " Batch 45: time=9.822s  loss=0.0994  iou=0.9484\n",
      " Batch 46: time=9.841s  loss=0.0848  iou=0.9673\n",
      " Batch 47: time=10.164s  loss=0.0878  iou=0.9676\n",
      " Batch 48: time=10.197s  loss=0.1031  iou=0.9496\n",
      " Batch 49: time=10.163s  loss=0.0972  iou=0.9521\n",
      " Batch 50: time=9.935s  loss=0.0972  iou=0.9503\n",
      " Batch 51: time=9.579s  loss=0.1141  iou=0.9251\n",
      " Batch 52: time=9.557s  loss=0.1127  iou=0.9506\n",
      " Batch 53: time=10.042s  loss=0.0969  iou=0.9526\n",
      " Batch 54: time=10.084s  loss=0.1001  iou=0.9467\n",
      " Batch 55: time=9.848s  loss=0.1137  iou=0.9337\n",
      " Batch 56: time=9.702s  loss=0.1216  iou=0.9086\n",
      " Batch 57: time=10.327s  loss=0.0911  iou=0.9573\n",
      " Batch 58: time=10.029s  loss=0.0920  iou=0.9546\n",
      " Batch 59: time=10.438s  loss=0.1010  iou=0.9379\n",
      " Batch 60: time=10.305s  loss=0.1101  iou=0.9233\n",
      " Batch 61: time=10.022s  loss=0.0878  iou=0.9675\n",
      " Batch 62: time=10.369s  loss=0.1060  iou=0.9577\n",
      " Batch 63: time=10.545s  loss=0.0949  iou=0.9536\n",
      " Batch 64: time=9.798s  loss=0.1015  iou=0.9477\n",
      " Batch 65: time=10.335s  loss=0.1256  iou=0.9029\n",
      " Batch 66: time=10.273s  loss=0.1119  iou=0.9370\n",
      " Batch 67: time=9.667s  loss=0.0977  iou=0.9520\n",
      " Batch 68: time=9.858s  loss=0.0863  iou=0.9669\n",
      " Batch 69: time=10.245s  loss=0.1027  iou=0.9403\n",
      " Batch 70: time=10.819s  loss=0.1019  iou=0.9441\n",
      " Batch 71: time=10.968s  loss=0.0970  iou=0.9421\n",
      " Batch 72: time=10.834s  loss=0.1177  iou=0.9355\n",
      " Batch 73: time=10.790s  loss=0.0788  iou=0.9751\n",
      " Batch 74: time=10.071s  loss=0.1046  iou=0.9261\n",
      " Batch 75: time=9.972s  loss=0.1016  iou=0.9327\n",
      " Batch 76: time=10.220s  loss=0.1031  iou=0.9280\n",
      " Batch 77: time=10.648s  loss=0.0937  iou=0.9499\n",
      " Batch 78: time=10.045s  loss=0.1127  iou=0.9347\n",
      " Batch 79: time=9.999s  loss=0.1068  iou=0.9410\n",
      " Batch 80: time=9.957s  loss=0.1182  iou=0.9462\n",
      " Batch 81: time=9.559s  loss=0.1308  iou=0.9064\n",
      " Batch 82: time=9.923s  loss=0.0948  iou=0.9406\n",
      " Batch 83: time=10.335s  loss=0.1060  iou=0.9468\n",
      " Batch 84: time=10.118s  loss=0.1017  iou=0.9390\n",
      " Batch 85: time=10.432s  loss=0.0947  iou=0.9470\n",
      " Batch 86: time=11.100s  loss=0.1398  iou=0.9218\n",
      " Batch 87: time=10.902s  loss=0.1116  iou=0.9184\n",
      " Batch 88: time=10.106s  loss=0.0914  iou=0.9512\n",
      " Batch 89: time=9.552s  loss=0.0939  iou=0.9495\n",
      " Batch 90: time=9.761s  loss=0.1355  iou=0.9304\n",
      " Batch 91: time=9.844s  loss=0.0766  iou=0.9799\n",
      " Batch 92: time=10.621s  loss=0.1208  iou=0.9003\n",
      " Batch 93: time=10.575s  loss=0.0949  iou=0.9403\n",
      " Batch 94: time=10.012s  loss=0.1106  iou=0.9149\n",
      " Batch 95: time=10.442s  loss=0.1023  iou=0.9367\n",
      " Batch 96: time=10.291s  loss=0.1439  iou=0.8654\n",
      " Batch 97: time=10.288s  loss=0.0825  iou=0.9579\n",
      " Batch 98: time=10.480s  loss=0.0833  iou=0.9728\n",
      " Batch 99: time=10.519s  loss=0.1026  iou=0.9316\n",
      " Batch 100: time=10.456s  loss=0.1531  iou=0.8991\n",
      " Batch 101: time=10.333s  loss=0.2592  iou=0.6391\n",
      " Batch 102: time=10.378s  loss=0.0841  iou=0.9649\n",
      " Batch 103: time=10.279s  loss=0.0995  iou=0.9387\n",
      " Batch 104: time=10.210s  loss=0.1586  iou=0.8351\n",
      " Batch 105: time=10.249s  loss=0.1210  iou=0.9024\n",
      " Batch 106: time=10.338s  loss=0.1390  iou=0.9246\n",
      " Batch 107: time=10.521s  loss=0.0930  iou=0.9500\n",
      " Batch 108: time=10.546s  loss=0.1167  iou=0.9140\n",
      " Batch 109: time=10.408s  loss=0.1200  iou=0.9164\n",
      " Batch 110: time=10.416s  loss=0.1262  iou=0.8992\n",
      " Batch 111: time=10.448s  loss=0.0841  iou=0.9575\n",
      " Batch 112: time=10.328s  loss=0.1177  iou=0.8961\n",
      " Batch 113: time=5.275s  loss=0.1329  iou=0.9301\n",
      "\n",
      "Epoch 7 Completed:\n",
      "  Time: 1313.86 sec\n",
      "  Train Loss: 0.1092 | Train IoU: 0.9349\n",
      "  Val Loss:   0.1020 | Val IoU:   0.9319\n",
      "\n",
      "================= EPOCH 8/10 =================\n",
      " Batch 1: time=10.386s  loss=0.1007  iou=0.9372\n",
      " Batch 2: time=11.452s  loss=0.0954  iou=0.9329\n",
      " Batch 3: time=10.494s  loss=0.1038  iou=0.9319\n",
      " Batch 4: time=10.384s  loss=0.1145  iou=0.9016\n",
      " Batch 5: time=10.387s  loss=0.0986  iou=0.9253\n",
      " Batch 6: time=10.385s  loss=0.0874  iou=0.9519\n",
      " Batch 7: time=10.161s  loss=0.1240  iou=0.8832\n",
      " Batch 8: time=10.179s  loss=0.0907  iou=0.9488\n",
      " Batch 9: time=10.164s  loss=0.0861  iou=0.9476\n",
      " Batch 10: time=10.491s  loss=0.0894  iou=0.9493\n",
      " Batch 11: time=10.463s  loss=0.0907  iou=0.9428\n",
      " Batch 12: time=10.258s  loss=0.0856  iou=0.9607\n",
      " Batch 13: time=10.296s  loss=0.0916  iou=0.9461\n",
      " Batch 14: time=10.247s  loss=0.0921  iou=0.9490\n",
      " Batch 15: time=10.322s  loss=0.0897  iou=0.9480\n",
      " Batch 16: time=10.230s  loss=0.0962  iou=0.9374\n",
      " Batch 17: time=10.183s  loss=0.1257  iou=0.9090\n",
      " Batch 18: time=10.255s  loss=0.1068  iou=0.9523\n",
      " Batch 19: time=10.231s  loss=0.0952  iou=0.9309\n",
      " Batch 20: time=10.201s  loss=0.0947  iou=0.9372\n",
      " Batch 21: time=10.286s  loss=0.0852  iou=0.9545\n",
      " Batch 22: time=10.337s  loss=0.0814  iou=0.9582\n",
      " Batch 23: time=10.313s  loss=0.0807  iou=0.9625\n",
      " Batch 24: time=10.091s  loss=0.0797  iou=0.9605\n",
      " Batch 25: time=9.818s  loss=0.0896  iou=0.9484\n",
      " Batch 26: time=9.852s  loss=0.0976  iou=0.9441\n",
      " Batch 27: time=9.778s  loss=0.0915  iou=0.9432\n",
      " Batch 28: time=9.731s  loss=0.0902  iou=0.9381\n",
      " Batch 29: time=10.368s  loss=0.0749  iou=0.9727\n",
      " Batch 30: time=9.914s  loss=0.1590  iou=0.8752\n",
      " Batch 31: time=9.982s  loss=0.0935  iou=0.9423\n",
      " Batch 32: time=9.968s  loss=0.0940  iou=0.9385\n",
      " Batch 33: time=9.962s  loss=0.0892  iou=0.9503\n",
      " Batch 34: time=10.036s  loss=0.1143  iou=0.9319\n",
      " Batch 35: time=9.977s  loss=0.1042  iou=0.9312\n",
      " Batch 36: time=10.003s  loss=0.0894  iou=0.9537\n",
      " Batch 37: time=9.992s  loss=0.1107  iou=0.9213\n",
      " Batch 38: time=10.096s  loss=0.0842  iou=0.9574\n",
      " Batch 39: time=10.060s  loss=0.1050  iou=0.9262\n",
      " Batch 40: time=10.078s  loss=0.1197  iou=0.9079\n",
      " Batch 41: time=9.953s  loss=0.0925  iou=0.9424\n",
      " Batch 42: time=10.007s  loss=0.1034  iou=0.9506\n",
      " Batch 43: time=10.122s  loss=0.1065  iou=0.9300\n",
      " Batch 44: time=10.095s  loss=0.1172  iou=0.9311\n",
      " Batch 45: time=10.287s  loss=0.0905  iou=0.9500\n",
      " Batch 46: time=10.319s  loss=0.0853  iou=0.9606\n",
      " Batch 47: time=10.307s  loss=0.0866  iou=0.9634\n",
      " Batch 48: time=10.231s  loss=0.1010  iou=0.9537\n",
      " Batch 49: time=10.222s  loss=0.1053  iou=0.9369\n",
      " Batch 50: time=10.432s  loss=0.0886  iou=0.9490\n",
      " Batch 51: time=11.056s  loss=0.0884  iou=0.9463\n",
      " Batch 52: time=11.219s  loss=0.0933  iou=0.9447\n",
      " Batch 53: time=11.026s  loss=0.0826  iou=0.9500\n",
      " Batch 54: time=11.117s  loss=0.1569  iou=0.8801\n",
      " Batch 55: time=11.125s  loss=0.0866  iou=0.9542\n",
      " Batch 56: time=11.165s  loss=0.0876  iou=0.9427\n",
      " Batch 57: time=11.150s  loss=0.0872  iou=0.9545\n",
      " Batch 58: time=11.081s  loss=0.0968  iou=0.9388\n",
      " Batch 59: time=11.191s  loss=0.0801  iou=0.9577\n",
      " Batch 60: time=11.081s  loss=0.0852  iou=0.9511\n",
      " Batch 61: time=11.122s  loss=0.1016  iou=0.9294\n",
      " Batch 62: time=11.196s  loss=0.0922  iou=0.9365\n",
      " Batch 63: time=11.135s  loss=0.0916  iou=0.9450\n",
      " Batch 64: time=11.175s  loss=0.0806  iou=0.9605\n",
      " Batch 65: time=11.128s  loss=0.0962  iou=0.9445\n",
      " Batch 66: time=11.252s  loss=0.0794  iou=0.9625\n",
      " Batch 67: time=11.157s  loss=0.1268  iou=0.9455\n",
      " Batch 68: time=11.109s  loss=0.0965  iou=0.9432\n",
      " Batch 69: time=11.149s  loss=0.0958  iou=0.9565\n",
      " Batch 70: time=11.239s  loss=0.0840  iou=0.9630\n",
      " Batch 71: time=11.178s  loss=0.1063  iou=0.9448\n",
      " Batch 72: time=11.069s  loss=0.0893  iou=0.9730\n",
      " Batch 73: time=11.168s  loss=0.0949  iou=0.9450\n",
      " Batch 74: time=11.194s  loss=0.1067  iou=0.9167\n",
      " Batch 75: time=11.199s  loss=0.0817  iou=0.9541\n",
      " Batch 76: time=11.073s  loss=0.1007  iou=0.9436\n",
      " Batch 77: time=11.215s  loss=0.0952  iou=0.9282\n",
      " Batch 78: time=11.158s  loss=0.0814  iou=0.9531\n",
      " Batch 79: time=11.214s  loss=0.1206  iou=0.9173\n",
      " Batch 80: time=11.669s  loss=0.0739  iou=0.9675\n",
      " Batch 81: time=11.248s  loss=0.0727  iou=0.9694\n",
      " Batch 82: time=11.155s  loss=0.0891  iou=0.9653\n",
      " Batch 83: time=11.160s  loss=0.0911  iou=0.9326\n",
      " Batch 84: time=11.154s  loss=0.1060  iou=0.9146\n",
      " Batch 85: time=11.224s  loss=0.0829  iou=0.9549\n",
      " Batch 86: time=11.179s  loss=0.1270  iou=0.9125\n",
      " Batch 87: time=11.157s  loss=0.0844  iou=0.9625\n",
      " Batch 88: time=11.156s  loss=0.0870  iou=0.9443\n",
      " Batch 89: time=11.195s  loss=0.0805  iou=0.9564\n",
      " Batch 90: time=11.151s  loss=0.0809  iou=0.9580\n",
      " Batch 91: time=11.136s  loss=0.0986  iou=0.9268\n",
      " Batch 92: time=11.117s  loss=0.0844  iou=0.9446\n",
      " Batch 93: time=11.164s  loss=0.0996  iou=0.9470\n",
      " Batch 94: time=10.967s  loss=0.0874  iou=0.9520\n",
      " Batch 95: time=11.166s  loss=0.0983  iou=0.9581\n",
      " Batch 96: time=11.141s  loss=0.0768  iou=0.9621\n",
      " Batch 97: time=11.158s  loss=0.0804  iou=0.9660\n",
      " Batch 98: time=11.249s  loss=0.1080  iou=0.9310\n",
      " Batch 99: time=11.246s  loss=0.0775  iou=0.9609\n",
      " Batch 100: time=11.089s  loss=0.0902  iou=0.9518\n",
      " Batch 101: time=11.143s  loss=0.0775  iou=0.9700\n",
      " Batch 102: time=11.120s  loss=0.0704  iou=0.9665\n",
      " Batch 103: time=11.171s  loss=0.0904  iou=0.9556\n",
      " Batch 104: time=11.171s  loss=0.0819  iou=0.9568\n",
      " Batch 105: time=11.159s  loss=0.1009  iou=0.9138\n",
      " Batch 106: time=11.163s  loss=0.0901  iou=0.9321\n",
      " Batch 107: time=11.184s  loss=0.0902  iou=0.9322\n",
      " Batch 108: time=11.145s  loss=0.0839  iou=0.9473\n",
      " Batch 109: time=11.194s  loss=0.1097  iou=0.9173\n",
      " Batch 110: time=11.130s  loss=0.0956  iou=0.9348\n",
      " Batch 111: time=11.160s  loss=0.0852  iou=0.9388\n",
      " Batch 112: time=11.069s  loss=0.0816  iou=0.9526\n",
      " Batch 113: time=5.612s  loss=0.1123  iou=0.9087\n",
      "\n",
      "Epoch 8 Completed:\n",
      "  Time: 1367.78 sec\n",
      "  Train Loss: 0.0945 | Train IoU: 0.9430\n",
      "  Val Loss:   0.0926 | Val IoU:   0.9394\n",
      "\n",
      "================= EPOCH 9/10 =================\n",
      " Batch 1: time=11.127s  loss=0.0893  iou=0.9384\n",
      " Batch 2: time=11.161s  loss=0.0950  iou=0.9476\n",
      " Batch 3: time=11.142s  loss=0.0773  iou=0.9656\n",
      " Batch 4: time=11.221s  loss=0.0889  iou=0.9594\n",
      " Batch 5: time=11.164s  loss=0.0867  iou=0.9448\n",
      " Batch 6: time=11.161s  loss=0.0830  iou=0.9507\n",
      " Batch 7: time=11.203s  loss=0.1141  iou=0.9141\n",
      " Batch 8: time=11.234s  loss=0.0751  iou=0.9590\n",
      " Batch 9: time=11.182s  loss=0.0785  iou=0.9531\n",
      " Batch 10: time=11.112s  loss=0.0927  iou=0.9501\n",
      " Batch 11: time=11.155s  loss=0.0896  iou=0.9361\n",
      " Batch 12: time=11.065s  loss=0.0678  iou=0.9703\n",
      " Batch 13: time=11.184s  loss=0.0772  iou=0.9600\n",
      " Batch 14: time=11.160s  loss=0.0848  iou=0.9464\n",
      " Batch 15: time=11.088s  loss=0.0929  iou=0.9360\n",
      " Batch 16: time=11.154s  loss=0.0982  iou=0.9367\n",
      " Batch 17: time=11.163s  loss=0.0749  iou=0.9562\n",
      " Batch 18: time=11.194s  loss=0.1072  iou=0.9251\n",
      " Batch 19: time=11.209s  loss=0.0770  iou=0.9625\n",
      " Batch 20: time=11.138s  loss=0.0721  iou=0.9638\n",
      " Batch 21: time=11.131s  loss=0.0797  iou=0.9735\n",
      " Batch 22: time=11.046s  loss=0.0837  iou=0.9466\n",
      " Batch 23: time=10.990s  loss=0.0745  iou=0.9601\n",
      " Batch 24: time=11.086s  loss=0.0877  iou=0.9615\n",
      " Batch 25: time=10.959s  loss=0.1387  iou=0.9330\n",
      " Batch 26: time=11.191s  loss=0.0712  iou=0.9702\n",
      " Batch 27: time=11.268s  loss=0.0890  iou=0.9476\n",
      " Batch 28: time=11.272s  loss=0.0862  iou=0.9445\n",
      " Batch 29: time=11.143s  loss=0.0705  iou=0.9687\n",
      " Batch 30: time=11.106s  loss=0.0870  iou=0.9353\n",
      " Batch 31: time=11.237s  loss=0.0743  iou=0.9591\n",
      " Batch 32: time=11.169s  loss=0.0801  iou=0.9540\n",
      " Batch 33: time=11.165s  loss=0.0744  iou=0.9633\n",
      " Batch 34: time=11.228s  loss=0.0699  iou=0.9694\n",
      " Batch 35: time=11.148s  loss=0.1207  iou=0.9399\n",
      " Batch 36: time=11.180s  loss=0.0677  iou=0.9700\n",
      " Batch 37: time=11.140s  loss=0.0706  iou=0.9610\n",
      " Batch 38: time=11.157s  loss=0.0826  iou=0.9452\n",
      " Batch 39: time=11.162s  loss=0.0873  iou=0.9464\n",
      " Batch 40: time=11.088s  loss=0.0673  iou=0.9692\n",
      " Batch 41: time=11.063s  loss=0.0991  iou=0.9362\n",
      " Batch 42: time=11.133s  loss=0.0809  iou=0.9481\n",
      " Batch 43: time=11.140s  loss=0.0777  iou=0.9471\n",
      " Batch 44: time=11.197s  loss=0.0793  iou=0.9553\n",
      " Batch 45: time=11.194s  loss=0.0802  iou=0.9502\n",
      " Batch 46: time=11.247s  loss=0.0719  iou=0.9651\n",
      " Batch 47: time=11.137s  loss=0.1007  iou=0.9183\n",
      " Batch 48: time=11.189s  loss=0.0676  iou=0.9681\n",
      " Batch 49: time=11.162s  loss=0.0637  iou=0.9733\n",
      " Batch 50: time=11.241s  loss=0.0739  iou=0.9590\n",
      " Batch 51: time=11.189s  loss=0.0802  iou=0.9462\n",
      " Batch 52: time=11.159s  loss=0.0777  iou=0.9526\n",
      " Batch 53: time=11.191s  loss=0.0683  iou=0.9649\n",
      " Batch 54: time=11.191s  loss=0.1129  iou=0.9293\n",
      " Batch 55: time=11.141s  loss=0.0942  iou=0.9469\n",
      " Batch 56: time=11.247s  loss=0.1128  iou=0.9325\n",
      " Batch 57: time=11.220s  loss=0.0927  iou=0.9675\n",
      " Batch 58: time=11.149s  loss=0.0728  iou=0.9672\n",
      " Batch 59: time=11.166s  loss=0.0856  iou=0.9601\n",
      " Batch 60: time=11.191s  loss=0.1250  iou=0.9289\n",
      " Batch 61: time=11.191s  loss=0.0698  iou=0.9671\n",
      " Batch 62: time=11.702s  loss=0.0748  iou=0.9533\n",
      " Batch 63: time=11.169s  loss=0.0648  iou=0.9715\n",
      " Batch 64: time=11.214s  loss=0.0805  iou=0.9614\n",
      " Batch 65: time=11.230s  loss=0.0852  iou=0.9324\n",
      " Batch 66: time=11.173s  loss=0.1408  iou=0.9028\n",
      " Batch 67: time=11.170s  loss=0.0659  iou=0.9669\n",
      " Batch 68: time=11.252s  loss=0.0804  iou=0.9598\n",
      " Batch 69: time=11.216s  loss=0.0756  iou=0.9461\n",
      " Batch 70: time=11.196s  loss=0.0830  iou=0.9470\n",
      " Batch 71: time=11.145s  loss=0.0804  iou=0.9464\n",
      " Batch 72: time=11.194s  loss=0.0691  iou=0.9623\n",
      " Batch 73: time=11.115s  loss=0.0672  iou=0.9710\n",
      " Batch 74: time=11.168s  loss=0.0957  iou=0.9421\n",
      " Batch 75: time=11.214s  loss=0.0759  iou=0.9643\n",
      " Batch 76: time=11.231s  loss=0.0670  iou=0.9661\n",
      " Batch 77: time=11.152s  loss=0.1068  iou=0.9289\n",
      " Batch 78: time=11.104s  loss=0.1050  iou=0.9178\n",
      " Batch 79: time=11.182s  loss=0.0639  iou=0.9727\n",
      " Batch 80: time=11.201s  loss=0.0579  iou=0.9793\n",
      " Batch 81: time=11.168s  loss=0.0691  iou=0.9654\n",
      " Batch 82: time=11.264s  loss=0.0838  iou=0.9522\n",
      " Batch 83: time=11.078s  loss=0.0722  iou=0.9586\n",
      " Batch 84: time=11.039s  loss=0.0704  iou=0.9614\n",
      " Batch 85: time=11.232s  loss=0.0705  iou=0.9625\n",
      " Batch 86: time=11.171s  loss=0.0854  iou=0.9370\n",
      " Batch 87: time=11.243s  loss=0.0750  iou=0.9521\n",
      " Batch 88: time=11.148s  loss=0.0701  iou=0.9633\n",
      " Batch 89: time=11.129s  loss=0.0742  iou=0.9557\n",
      " Batch 90: time=11.191s  loss=0.0801  iou=0.9413\n",
      " Batch 91: time=11.244s  loss=0.0691  iou=0.9642\n",
      " Batch 92: time=11.121s  loss=0.0799  iou=0.9664\n",
      " Batch 93: time=11.199s  loss=0.0718  iou=0.9662\n",
      " Batch 94: time=11.193s  loss=0.0757  iou=0.9581\n",
      " Batch 95: time=11.208s  loss=0.0714  iou=0.9698\n",
      " Batch 96: time=11.149s  loss=0.0723  iou=0.9596\n",
      " Batch 97: time=11.162s  loss=0.0785  iou=0.9650\n",
      " Batch 98: time=11.176s  loss=0.0969  iou=0.9593\n",
      " Batch 99: time=11.032s  loss=0.0700  iou=0.9610\n",
      " Batch 100: time=11.031s  loss=0.0858  iou=0.9325\n",
      " Batch 101: time=11.137s  loss=0.0690  iou=0.9632\n",
      " Batch 102: time=11.088s  loss=0.0750  iou=0.9541\n",
      " Batch 103: time=11.124s  loss=0.0668  iou=0.9668\n",
      " Batch 104: time=11.153s  loss=0.0903  iou=0.9423\n",
      " Batch 105: time=11.240s  loss=0.0619  iou=0.9700\n",
      " Batch 106: time=11.234s  loss=0.0787  iou=0.9611\n",
      " Batch 107: time=11.166s  loss=0.0737  iou=0.9491\n",
      " Batch 108: time=11.197s  loss=0.0813  iou=0.9391\n",
      " Batch 109: time=11.169s  loss=0.0712  iou=0.9601\n",
      " Batch 110: time=11.188s  loss=0.0650  iou=0.9743\n",
      " Batch 111: time=11.170s  loss=0.0732  iou=0.9705\n",
      " Batch 112: time=11.772s  loss=0.0682  iou=0.9694\n",
      " Batch 113: time=5.675s  loss=0.0720  iou=0.9575\n",
      "\n",
      "Epoch 9 Completed:\n",
      "  Time: 1420.59 sec\n",
      "  Train Loss: 0.0812 | Train IoU: 0.9540\n",
      "  Val Loss:   0.0715 | Val IoU:   0.9587\n",
      "\n",
      "================= EPOCH 10/10 =================\n",
      " Batch 1: time=11.198s  loss=0.0720  iou=0.9580\n",
      " Batch 2: time=11.251s  loss=0.0771  iou=0.9519\n",
      " Batch 3: time=11.151s  loss=0.0783  iou=0.9500\n",
      " Batch 4: time=11.221s  loss=0.0804  iou=0.9474\n",
      " Batch 5: time=11.092s  loss=0.0664  iou=0.9706\n",
      " Batch 6: time=11.210s  loss=0.0786  iou=0.9563\n",
      " Batch 7: time=11.152s  loss=0.1079  iou=0.9241\n",
      " Batch 8: time=11.226s  loss=0.0973  iou=0.9622\n",
      " Batch 9: time=11.205s  loss=0.0789  iou=0.9659\n",
      " Batch 10: time=11.265s  loss=0.0735  iou=0.9586\n",
      " Batch 11: time=11.250s  loss=0.0667  iou=0.9633\n",
      " Batch 12: time=11.293s  loss=0.1010  iou=0.9498\n",
      " Batch 13: time=11.364s  loss=0.0704  iou=0.9553\n",
      " Batch 14: time=11.199s  loss=0.0730  iou=0.9657\n",
      " Batch 15: time=11.200s  loss=0.0851  iou=0.9568\n",
      " Batch 16: time=11.163s  loss=0.0800  iou=0.9532\n",
      " Batch 17: time=11.342s  loss=0.0692  iou=0.9682\n",
      " Batch 18: time=11.252s  loss=0.0673  iou=0.9625\n",
      " Batch 19: time=11.189s  loss=0.0610  iou=0.9725\n",
      " Batch 20: time=11.234s  loss=0.0714  iou=0.9521\n",
      " Batch 21: time=11.218s  loss=0.0773  iou=0.9572\n",
      " Batch 22: time=11.223s  loss=0.0787  iou=0.9495\n",
      " Batch 23: time=11.261s  loss=0.0929  iou=0.9024\n",
      " Batch 24: time=11.218s  loss=0.0728  iou=0.9530\n",
      " Batch 25: time=11.102s  loss=0.0642  iou=0.9647\n",
      " Batch 26: time=11.189s  loss=0.0668  iou=0.9606\n",
      " Batch 27: time=11.162s  loss=0.0682  iou=0.9623\n",
      " Batch 28: time=11.169s  loss=0.0772  iou=0.9447\n",
      " Batch 29: time=11.238s  loss=0.0680  iou=0.9587\n",
      " Batch 30: time=11.298s  loss=0.0760  iou=0.9536\n",
      " Batch 31: time=11.247s  loss=0.0610  iou=0.9744\n",
      " Batch 32: time=11.205s  loss=0.0800  iou=0.9534\n",
      " Batch 33: time=11.069s  loss=0.0830  iou=0.9577\n",
      " Batch 34: time=11.221s  loss=0.0648  iou=0.9681\n",
      " Batch 35: time=11.171s  loss=0.0990  iou=0.9172\n",
      " Batch 36: time=11.200s  loss=0.0773  iou=0.9374\n",
      " Batch 37: time=11.166s  loss=0.0687  iou=0.9590\n",
      " Batch 38: time=11.112s  loss=0.0650  iou=0.9710\n",
      " Batch 39: time=11.107s  loss=0.0957  iou=0.9433\n",
      " Batch 40: time=11.270s  loss=0.0777  iou=0.9316\n",
      " Batch 41: time=11.136s  loss=0.0771  iou=0.9422\n",
      " Batch 42: time=11.163s  loss=0.0893  iou=0.9383\n",
      " Batch 43: time=11.171s  loss=0.0859  iou=0.9280\n",
      " Batch 44: time=11.700s  loss=0.0747  iou=0.9653\n",
      " Batch 45: time=11.149s  loss=0.1213  iou=0.8459\n",
      " Batch 46: time=11.102s  loss=0.0842  iou=0.9329\n",
      " Batch 47: time=11.280s  loss=0.1087  iou=0.8910\n",
      " Batch 48: time=11.146s  loss=0.0754  iou=0.9577\n",
      " Batch 49: time=11.182s  loss=0.0632  iou=0.9614\n",
      " Batch 50: time=11.160s  loss=0.0681  iou=0.9612\n",
      " Batch 51: time=11.205s  loss=0.0815  iou=0.9359\n",
      " Batch 52: time=11.167s  loss=0.0805  iou=0.9286\n",
      " Batch 53: time=11.141s  loss=0.0881  iou=0.9153\n",
      " Batch 54: time=11.181s  loss=0.0716  iou=0.9505\n",
      " Batch 55: time=11.213s  loss=0.0770  iou=0.9381\n",
      " Batch 56: time=11.224s  loss=0.0720  iou=0.9532\n",
      " Batch 57: time=11.151s  loss=0.0921  iou=0.9273\n",
      " Batch 58: time=11.173s  loss=0.1014  iou=0.9005\n",
      " Batch 59: time=11.165s  loss=0.0707  iou=0.9495\n",
      " Batch 60: time=11.056s  loss=0.1024  iou=0.9264\n",
      " Batch 61: time=11.196s  loss=0.0664  iou=0.9581\n",
      " Batch 62: time=11.179s  loss=0.0854  iou=0.9534\n",
      " Batch 63: time=11.087s  loss=0.0638  iou=0.9625\n",
      " Batch 64: time=11.122s  loss=0.0895  iou=0.9426\n",
      " Batch 65: time=11.067s  loss=0.0760  iou=0.9377\n",
      " Batch 66: time=11.138s  loss=0.0889  iou=0.9457\n",
      " Batch 67: time=11.210s  loss=0.0783  iou=0.9457\n",
      " Batch 68: time=11.112s  loss=0.0669  iou=0.9559\n",
      " Batch 69: time=12.169s  loss=0.0650  iou=0.9630\n",
      " Batch 70: time=11.176s  loss=0.0951  iou=0.9325\n",
      " Batch 71: time=11.135s  loss=0.0869  iou=0.9576\n",
      " Batch 72: time=11.206s  loss=0.0722  iou=0.9689\n",
      " Batch 73: time=11.235s  loss=0.0856  iou=0.9441\n",
      " Batch 74: time=11.220s  loss=0.0707  iou=0.9522\n",
      " Batch 75: time=11.159s  loss=0.0674  iou=0.9588\n",
      " Batch 76: time=11.188s  loss=0.0651  iou=0.9594\n",
      " Batch 77: time=11.173s  loss=0.0710  iou=0.9530\n",
      " Batch 78: time=11.230s  loss=0.0670  iou=0.9715\n",
      " Batch 79: time=11.191s  loss=0.0682  iou=0.9548\n",
      " Batch 80: time=11.239s  loss=0.0685  iou=0.9517\n",
      " Batch 81: time=11.163s  loss=0.0624  iou=0.9684\n",
      " Batch 82: time=11.186s  loss=0.0650  iou=0.9679\n",
      " Batch 83: time=11.102s  loss=0.0662  iou=0.9622\n",
      " Batch 84: time=11.101s  loss=0.0563  iou=0.9758\n",
      " Batch 85: time=11.114s  loss=0.0662  iou=0.9582\n",
      " Batch 86: time=10.978s  loss=0.0835  iou=0.9315\n",
      " Batch 87: time=11.216s  loss=0.0855  iou=0.9202\n",
      " Batch 88: time=11.202s  loss=0.0632  iou=0.9601\n",
      " Batch 89: time=11.210s  loss=0.0658  iou=0.9603\n",
      " Batch 90: time=11.207s  loss=0.0767  iou=0.9663\n",
      " Batch 91: time=11.069s  loss=0.0643  iou=0.9575\n",
      " Batch 92: time=11.137s  loss=0.0714  iou=0.9443\n",
      " Batch 93: time=11.831s  loss=0.0563  iou=0.9763\n",
      " Batch 94: time=11.186s  loss=0.0612  iou=0.9720\n",
      " Batch 95: time=11.224s  loss=0.0494  iou=0.9833\n",
      " Batch 96: time=11.113s  loss=0.1135  iou=0.9039\n",
      " Batch 97: time=11.141s  loss=0.0615  iou=0.9691\n",
      " Batch 98: time=11.096s  loss=0.0872  iou=0.9121\n",
      " Batch 99: time=11.242s  loss=0.0790  iou=0.9405\n",
      " Batch 100: time=11.125s  loss=0.0582  iou=0.9688\n",
      " Batch 101: time=11.222s  loss=0.0626  iou=0.9642\n",
      " Batch 102: time=11.026s  loss=0.0686  iou=0.9483\n",
      " Batch 103: time=11.234s  loss=0.0950  iou=0.9279\n",
      " Batch 104: time=11.155s  loss=0.0669  iou=0.9657\n",
      " Batch 105: time=11.203s  loss=0.0825  iou=0.9547\n",
      " Batch 106: time=11.120s  loss=0.0586  iou=0.9700\n",
      " Batch 107: time=11.231s  loss=0.0985  iou=0.9200\n",
      " Batch 108: time=11.229s  loss=0.0912  iou=0.9428\n",
      " Batch 109: time=11.275s  loss=0.0728  iou=0.9501\n",
      " Batch 110: time=11.134s  loss=0.0531  iou=0.9793\n",
      " Batch 111: time=11.099s  loss=0.0638  iou=0.9599\n",
      " Batch 112: time=11.101s  loss=0.0544  iou=0.9775\n",
      " Batch 113: time=5.591s  loss=0.0659  iou=0.9619\n",
      "\n",
      "Epoch 10 Completed:\n",
      "  Time: 1423.25 sec\n",
      "  Train Loss: 0.0759 | Train IoU: 0.9505\n",
      "  Val Loss:   0.0695 | Val IoU:   0.9571\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL SIMPLE TRAINING LOOP WITH TIME PRINTING =====\n",
    "import time\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "scaler = GradScaler()\n",
    "\n",
    "def compute_iou(pred, target, thr=0.5, eps=1e-7):\n",
    "    p = (pred >= thr).float()\n",
    "    t = (target >= thr).float()\n",
    "    inter = (p * t).sum()\n",
    "    union = ((p + t) >= 1).float().sum()\n",
    "    return float((inter + eps) / (union + eps))\n",
    "\n",
    "history = {\"train_loss\":[], \"val_loss\":[], \"epoch_time\":[], \"train_iou\":[], \"val_iou\":[]}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(f\"\\n================= EPOCH {epoch}/{NUM_EPOCHS} =================\")\n",
    "\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        t0 = time.time()\n",
    "\n",
    "        inp, comp, bg, alpha, fg = prepare_input(batch)\n",
    "\n",
    "        with autocast():\n",
    "            pred = model(inp)\n",
    "            loss, l_alpha, l_comp = alpha_composition_loss(pred, alpha, fg, bg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        iou = compute_iou(pred.detach(), alpha.detach())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_iou += iou\n",
    "        batches += 1\n",
    "\n",
    "        print(f\" Batch {batches}: time={time.time()-t0:.3f}s  loss={loss.item():.4f}  iou={iou:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / batches\n",
    "    train_iou = running_iou / batches\n",
    "\n",
    "    # -------- Validation --------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_iou = 0.0\n",
    "    vb = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inp, comp, bg, alpha, fg = prepare_input(batch)\n",
    "            with autocast():\n",
    "                pred = model(inp)\n",
    "                loss, l_alpha, l_comp = alpha_composition_loss(pred, alpha, fg, bg)\n",
    "\n",
    "            iou = compute_iou(pred, alpha)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou\n",
    "            vb += 1\n",
    "\n",
    "    val_loss /= vb\n",
    "    val_iou /= vb\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_iou\"].append(train_iou)\n",
    "    history[\"val_iou\"].append(val_iou)\n",
    "    history[\"epoch_time\"].append(epoch_time)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} Completed:\")\n",
    "    print(f\"  Time: {epoch_time:.2f} sec\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train IoU: {train_iou:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val IoU:   {val_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69c524b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training history to c:\\Users\\PRABHAKAR\\Downloads\\matting_project\\checkpoints\\history.json\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "hist_path = SAVE_DIR / \"history.json\"\n",
    "with open(hist_path, \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "print(\"Saved training history to\", hist_path)\n",
    "\n",
    "plt.plot(history['train_loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17936b4d",
   "metadata": {},
   "source": [
    "**Inpanting Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750a2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686be4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PRABHAKAR\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports and device\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbca32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== USER PARAMETERS (EDIT THESE) =====\n",
    "DATASET_ID = r'C:\\Users\\PRABHAKAR\\Downloads\\Inpanting'   # e.g. \"username/dataset-name\"\n",
    "IMAGE_KEY  = \"image\"                        # the dataset field that contains image data\n",
    "SPLIT_NAMES = {\"train\":\"train\", \"val\":\"validation\", \"test\":\"test\"}  # change keys if needed\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 10\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "OUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "# ========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799ea6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask generation (random rectangle masks). Replace with your mask policy if you have ground-truth masks.\n",
    "def random_mask(img_hw, mask_ratio=0.25, min_size=32, max_rects=4):\n",
    "    h, w = img_hw\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    area_target = int(h * w * mask_ratio)\n",
    "    covered = 0\n",
    "    attempts = 0\n",
    "    while covered < area_target and attempts < 20:\n",
    "        rect_h = random.randint(min_size, max(min_size, h//3))\n",
    "        rect_w = random.randint(min_size, max(min_size, w//3))\n",
    "        top = random.randint(0, h - rect_h)\n",
    "        left = random.randint(0, w - rect_w)\n",
    "        mask[top:top+rect_h, left:left+rect_w] = 1\n",
    "        covered = mask.sum()\n",
    "        attempts += 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a84f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Dataset -> PyTorch Dataset wrapper\n",
    "class HFDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_dataset, split, image_key=\"image\", img_size=IMG_SIZE, transform=None):\n",
    "        self.ds = hf_dataset[split]\n",
    "        self.image_key = image_key\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def _load_image(self, img_field):\n",
    "        # handle several common HF image formats: PIL Image, numpy array, bytes, dict with path\n",
    "        if isinstance(img_field, Image.Image):\n",
    "            img = img_field.convert(\"RGB\")\n",
    "        elif isinstance(img_field, bytes):\n",
    "            img = Image.open(io.BytesIO(img_field)).convert(\"RGB\")\n",
    "        elif isinstance(img_field, dict):\n",
    "            # Example: {\"path\": \"...\"} or {\"bytes\": ...}\n",
    "            if \"path\" in img_field:\n",
    "                img = Image.open(img_field[\"path\"]).convert(\"RGB\")\n",
    "            elif \"bytes\" in img_field:\n",
    "                img = Image.open(io.BytesIO(img_field[\"bytes\"])).convert(\"RGB\")\n",
    "            else:\n",
    "                # fallback try to convert to array\n",
    "                img = Image.fromarray(np.asarray(img_field)).convert(\"RGB\")\n",
    "        else:\n",
    "            # numpy array or PIL convertible\n",
    "            img = Image.fromarray(np.asarray(img_field)).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        if self.image_key not in item:\n",
    "            raise KeyError(f\"Image key '{self.image_key}' not found in dataset item keys: {list(item.keys())[:10]}\")\n",
    "        raw_img = item[self.image_key]\n",
    "        img = self._load_image(raw_img)\n",
    "        img_t = self.transform(img)  # [C,H,W], float tensor 0..1\n",
    "\n",
    "        # mask & masked image\n",
    "        mask_np = random_mask((self.img_size, self.img_size), mask_ratio=0.25)\n",
    "        mask = torch.from_numpy(mask_np).unsqueeze(0).float()  # [1,H,W]\n",
    "        masked_img = img_t * (1 - mask)\n",
    "\n",
    "        return masked_img, img_t, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a267b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight UNet for inpainting\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=4, out_ch=3, base=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = DoubleConv(in_ch, base)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = DoubleConv(base, base*2)\n",
    "        self.enc3 = DoubleConv(base*2, base*4)\n",
    "        self.enc4 = DoubleConv(base*4, base*8)\n",
    "        self.bottleneck = DoubleConv(base*8, base*16)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(base*16, base*8, 2, stride=2)\n",
    "        self.dec4 = DoubleConv(base*16, base*8)\n",
    "        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(base*8, base*4)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(base*4, base*2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
    "        self.dec1 = DoubleConv(base*2, base)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        return torch.sigmoid(out)\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, out_channels=3, pretrained_resnet=None):\n",
    "        super().__init__()\n",
    "        if pretrained_resnet is None:\n",
    "            pretrained_resnet = models.resnet34(pretrained=True)\n",
    "        # Encoder parts from pretrained resnet34\n",
    "        # Note: conv1 -> bn1 -> relu produces x0 with 64 channels\n",
    "        self.inc_conv = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # copy pretrained weights for RGB channels, zero-init mask channel if n_channels==4\n",
    "        w = pretrained_resnet.conv1.weight.data.clone()\n",
    "        self.inc_conv.weight.data[:, :3, :, :] = w\n",
    "        if n_channels == 4:\n",
    "            self.inc_conv.weight.data[:, 3:, :, :].zero_()\n",
    "        self.inc_bn = pretrained_resnet.bn1\n",
    "        self.inc_relu = pretrained_resnet.relu\n",
    "        self.maxpool = pretrained_resnet.maxpool\n",
    "\n",
    "        self.enc1 = pretrained_resnet.layer1   # 64 channels\n",
    "        self.enc2 = pretrained_resnet.layer2   # 128 channels\n",
    "        self.enc3 = pretrained_resnet.layer3   # 256 channels\n",
    "        self.enc4 = pretrained_resnet.layer4   # 512 channels\n",
    "\n",
    "        # decoder conv blocks (decoders take concatenated inputs)\n",
    "        def dec_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        # upsample transpose convs to bring spatial resolution up\n",
    "        self.up_transpose0 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)  # e4 -> 256\n",
    "        self.dec4 = dec_block(256 + 256, 256)   # concat with e3 (256) -> input 512 -> out 256\n",
    "\n",
    "        self.up_transpose1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # 256 -> 128\n",
    "        self.dec3 = dec_block(128 + 128, 128)   # concat with e2 (128)\n",
    "\n",
    "        self.up_transpose2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)   # 128 -> 64\n",
    "        self.dec2 = dec_block(64 + 64, 64)      # concat with e1 (64)\n",
    "\n",
    "        self.up_transpose3 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)    # 64 -> 64 (upsample)\n",
    "        self.dec1 = dec_block(64 + 64, 64)      # concat with x0 (64)\n",
    "\n",
    "        self.outconv = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.inc_conv(x)       # [B,64,H/2,W/2]\n",
    "        x0 = self.inc_bn(x0)\n",
    "        x0 = self.inc_relu(x0)\n",
    "\n",
    "        x1 = self.maxpool(x0)       # [B,64,H/4,W/4]\n",
    "        e1 = self.enc1(x1)          # [B,64,H/4,W/4]\n",
    "        e2 = self.enc2(e1)          # [B,128,H/8,W/8]\n",
    "        e3 = self.enc3(e2)          # [B,256,H/16,W/16]\n",
    "        e4 = self.enc4(e3)          # [B,512,H/32,W/32]\n",
    "\n",
    "        # Decoder: upsample -> concat with corresponding encoder -> conv block\n",
    "        u4 = self.up_transpose0(e4)           # [B,256,H/16,W/16]\n",
    "        u4 = torch.cat([u4, e3], dim=1)       # [B,512,H/16,W/16]\n",
    "        u4 = self.dec4(u4)                    # [B,256,H/16,W/16]\n",
    "\n",
    "        u3 = self.up_transpose1(u4)           # [B,128,H/8,W/8]\n",
    "        u3 = torch.cat([u3, e2], dim=1)       # [B,256,H/8,W/8]\n",
    "        u3 = self.dec3(u3)                    # [B,128,H/8,W/8]\n",
    "\n",
    "        u2 = self.up_transpose2(u3)           # [B,64,H/4,W/4]\n",
    "        u2 = torch.cat([u2, e1], dim=1)       # [B,128,H/4,W/4]\n",
    "        u2 = self.dec2(u2)                    # [B,64,H/4,W/4]\n",
    "\n",
    "        u1 = self.up_transpose3(u2)           # [B,64,H/2,W/2]\n",
    "        # Note: x0 has shape [B,64,H/2,W/2], so concat is valid\n",
    "        u1 = torch.cat([u1, x0], dim=1)       # [B,128,H/2,W/2]\n",
    "        u1 = self.dec1(u1)                    # [B,64,H/2,W/2]\n",
    "\n",
    "        out = self.outconv(u1)                # [B,3,H/2,W/2] -> but we need full resolution\n",
    "        # If needed, upsample to original input size\n",
    "        out = F.interpolate(out, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099836d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HF dataset: C:\\Users\\PRABHAKAR\\Downloads\\Inpanting\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "Train size: 71953 Val size: 40504\n"
     ]
    }
   ],
   "source": [
    "# Load HF dataset and create loaders\n",
    "print(\"Loading HF dataset:\", DATASET_ID)\n",
    "hf_ds = load_dataset(DATASET_ID)\n",
    "print(\"Available splits:\", list(hf_ds.keys()))\n",
    "\n",
    "train_split = SPLIT_NAMES.get(\"train\", \"train\")\n",
    "val_split   = SPLIT_NAMES.get(\"val\", \"validation\")\n",
    "\n",
    "train_ds = HFDatasetWrapper(hf_ds, split=train_split, image_key=IMAGE_KEY, img_size=IMG_SIZE)\n",
    "val_ds   = HFDatasetWrapper(hf_ds, split=val_split, image_key=IMAGE_KEY, img_size=IMG_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f525567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model, loss, optimizer\n",
    "model = UNet(in_ch=4, out_ch=3, base=64).to(device)\n",
    "criterion = nn.L1Loss()   # pixel L1 on masked region\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904ab67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with IMG_SIZE=64, UNET_BASE=16, BATCH_SIZE=8, EPOCHS=5\n",
      "Train size: 71953 Val size: 40504\n"
     ]
    }
   ],
   "source": [
    "# ===== HYPERPARAMS (edit if needed) =====\n",
    "# Set EPOCHS = 5 or 7 here\n",
    "EPOCHS = 5   # change to 7 if you'd like more epochs\n",
    "\n",
    "IMG_SIZE = 64        # keep small for fast runs; increase later for better quality\n",
    "UNET_BASE = 16       # small model\n",
    "LR = 1e-4\n",
    "\n",
    "# batch size: larger on GPU, smaller on CPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 32\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "TRAIN_BATCHES_LIMIT = 150   # keep for quick runs; set to None to run full epoch\n",
    "VAL_BATCHES_LIMIT = 40\n",
    "\n",
    "OUT_DIR = \"./outputs_metrics\"\n",
    "import os\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Recreate datasets/loaders (assumes hf_ds and HFDatasetWrapper are already defined)\n",
    "train_split = \"train\"\n",
    "val_split = \"validation\"\n",
    "\n",
    "train_ds = HFDatasetWrapper(hf_ds, split=train_split, image_key=IMAGE_KEY, img_size=IMG_SIZE)\n",
    "val_ds   = HFDatasetWrapper(hf_ds, split=val_split,   image_key=IMAGE_KEY, img_size=IMG_SIZE)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "\n",
    "print(f\"Running with IMG_SIZE={IMG_SIZE}, UNET_BASE={UNET_BASE}, BATCH_SIZE={BATCH_SIZE}, EPOCHS={EPOCHS}\")\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680a3b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/8995 [00:00<?, ?it/s]C:\\Users\\PRABHAKAR\\AppData\\Local\\Temp\\ipykernel_20012\\965500912.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "Epoch 1/5:   2%|‚ñè         | 150/8995 [02:38<2:35:21,  1.05s/it, loss=0.0410, acc=311.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Train Loss: 0.041019\n",
      "Train Accuracy: 311.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   2%|‚ñè         | 150/8995 [02:57<2:54:47,  1.19s/it, loss=0.0374, acc=315.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Train Loss: 0.037360\n",
      "Train Accuracy: 315.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   2%|‚ñè         | 150/8995 [02:26<2:24:18,  1.02it/s, loss=0.0364, acc=317.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Train Loss: 0.036409\n",
      "Train Accuracy: 317.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   2%|‚ñè         | 150/8995 [02:22<2:20:11,  1.05it/s, loss=0.0359, acc=317.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Train Loss: 0.035850\n",
      "Train Accuracy: 317.68%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   2%|‚ñè         | 150/8995 [02:23<2:21:04,  1.04it/s, loss=0.0364, acc=318.18%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Train Loss: 0.036442\n",
      "Train Accuracy: 318.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== TRAINING WITH LIVE ACCURACY PRINTING =====\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# accuracy function (masked region)\n",
    "# -----------------------------\n",
    "def masked_pixel_accuracy(pred, target, mask, tol=0.05):\n",
    "    # pred,target: [B,3,H,W], mask: [B,1,H,W]\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    target = target.detach().cpu().numpy()\n",
    "    mask = mask.detach().cpu().numpy()[:,0]  # [B,H,W]\n",
    "\n",
    "    B = pred.shape[0]\n",
    "    accs = []\n",
    "    for i in range(B):\n",
    "        m = mask[i]\n",
    "        if m.sum() == 0:\n",
    "            accs.append(1.0)\n",
    "            continue\n",
    "        p = pred[i]\n",
    "        t = target[i]\n",
    "        abs_err = np.mean(np.abs(p - t), axis=0)  # [H,W]\n",
    "        masked_err = abs_err * m\n",
    "        correct = (masked_err <= tol).astype(np.float32)\n",
    "        acc = correct.sum() / (m.sum() + 1e-8)\n",
    "        accs.append(acc)\n",
    "    return float(np.mean(accs)) * 100.0\n",
    "\n",
    "# use mixed precision on GPU\n",
    "use_amp = torch.cuda.is_available()\n",
    "if use_amp:\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=True)\n",
    "\n",
    "    for i, (masked, target, mask) in enumerate(loop):\n",
    "        if TRAIN_BATCHES_LIMIT is not None and i >= TRAIN_BATCHES_LIMIT:\n",
    "            break\n",
    "\n",
    "        masked = masked.to(device)\n",
    "        target = target.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        inp = torch.cat([masked, mask], dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            pred = model(inp)\n",
    "            loss = criterion(pred * mask, target * mask)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # compute accuracy for this batch\n",
    "        batch_acc = masked_pixel_accuracy(pred, target, mask)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += batch_acc\n",
    "        steps += 1\n",
    "\n",
    "        # show live metrics in tqdm bar\n",
    "        loop.set_postfix({\n",
    "            \"loss\": f\"{epoch_loss/steps:.4f}\",\n",
    "            \"acc\": f\"{epoch_acc/steps:.2f}%\"\n",
    "        })\n",
    "\n",
    "    # print epoch summary\n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {epoch_loss/steps:.6f}\")\n",
    "    print(f\"Train Accuracy: {epoch_acc/steps:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5c4f356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   1%|          | 499/40775 [00:24<33:03, 20.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test outputs saved to: ./outputs_metrics\\test_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional: run inference on test split and save outputs (if test split exists)\n",
    "if SPLIT_NAMES.get(\"test\", \"test\") in hf_ds:\n",
    "    test_split = SPLIT_NAMES.get(\"test\", \"test\")\n",
    "    test_ds = HFDatasetWrapper(hf_ds, split=test_split, image_key=IMAGE_KEY, img_size=IMG_SIZE)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "    os.makedirs(os.path.join(OUT_DIR, \"test_outputs\"), exist_ok=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (masked, target, mask) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            inp = torch.cat([masked.to(device), mask.to(device)], dim=1)\n",
    "            pred = model(inp)\n",
    "            composite = masked.to(device) * (1 - mask.to(device)) + pred * mask.to(device)\n",
    "            save_image(composite.cpu(), os.path.join(OUT_DIR, \"test_outputs\", f\"{idx:05d}.png\"))\n",
    "            # limit to first 500 outputs to avoid huge directories - change as needed\n",
    "            if idx >= 499:\n",
    "                break\n",
    "    print(\"Test outputs saved to:\", os.path.join(OUT_DIR, \"test_outputs\"))\n",
    "else:\n",
    "    print(\"No test split found in dataset. Skipping test inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04823d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
